# ëª¨ë¸ ì–‘ìí™” ê°€ì´ë“œ (Quantization Guide)

ì´ ê°€ì´ë“œëŠ” í•™ìŠµëœ í•œêµ­ì–´ ì•…ì„± ëŒ“ê¸€ ë¶„ë¥˜ ëª¨ë¸ì„ 4-bit ì–‘ìí™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  ì¶”ë¡  ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ëª©ì°¨

1. [ì–‘ìí™” ê°œìš”](#ì–‘ìí™”-ê°œìš”)
2. [ì‚¬ì „ ì¤€ë¹„](#ì‚¬ì „-ì¤€ë¹„)
3. [ì–‘ìí™” ê³¼ì •](#ì–‘ìí™”-ê³¼ì •)
4. [ê²°ê³¼ ë¶„ì„](#ê²°ê³¼-ë¶„ì„)
5. [ì–‘ìí™”ëœ ëª¨ë¸ ì‚¬ìš©](#ì–‘ìí™”ëœ-ëª¨ë¸-ì‚¬ìš©)
6. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

## ì–‘ìí™” ê°œìš”

### ì–‘ìí™”ë€?

ì–‘ìí™”(Quantization)ëŠ” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë‚®ì€ ì •ë°€ë„ë¡œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ í¬ê¸°ë¥¼ ì¤„ì´ê³  ì¶”ë¡  ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

### 4-bit ì–‘ìí™”ì˜ ì¥ì 

- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ**: ëª¨ë¸ í¬ê¸° ì•½ 75% ê°ì†Œ
- **ì¶”ë¡  ì†ë„ í–¥ìƒ**: ë” ë¹ ë¥¸ ì˜ˆì¸¡
- **ë°°í¬ ìš©ì´ì„±**: ì‘ì€ ëª¨ë¸ë¡œ ì„œë²„ ë°°í¬ ê°€ëŠ¥
- **ë¹„ìš© ì ˆì•½**: ë” ì ì€ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ í•„ìš”

### ì–‘ìí™” ì „í›„ ë¹„êµ

| í•­ëª© | ì›ë³¸ ëª¨ë¸ | ì–‘ìí™”ëœ ëª¨ë¸ |
|------|-----------|---------------|
| ëª¨ë¸ í¬ê¸° | ~420MB | ~105MB |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | ~2GB | ~0.5GB |
| ì¶”ë¡  ì†ë„ | 100ms | 60ms |
| ì •í™•ë„ | 88.2% | 87.8% |

## ì‚¬ì „ ì¤€ë¹„

### 1. í•™ìŠµëœ ëª¨ë¸ í™•ì¸

ì–‘ìí™”ë¥¼ ì§„í–‰í•˜ê¸° ì „ì— í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤.

```bash
# í•™ìŠµëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ í™•ì¸
ls model-checkpoints/kobert/

# ì˜ˆìƒ ì¶œë ¥:
# checkpoint-500/
# checkpoint-1000/
# checkpoint-1100/
```

### 2. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# bitsandbytes ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (4-bit ì–‘ìí™”ìš©)
pip install bitsandbytes

# ê¸°íƒ€ ì˜ì¡´ì„± í™•ì¸
pip install -r requirements.txt
```

### 3. GPU ë©”ëª¨ë¦¬ í™•ì¸

```bash
# GPU ë©”ëª¨ë¦¬ í™•ì¸
nvidia-smi

# ìµœì†Œ 4GB GPU ë©”ëª¨ë¦¬ ê¶Œì¥
```

## ì–‘ìí™” ê³¼ì •

### 1. ê¸°ë³¸ ì–‘ìí™” ì‹¤í–‰

```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì–‘ìí™”
python quantization.py --checkpoint_path model-checkpoints/kobert/checkpoint-1100

# ì»¤ìŠ¤í…€ ì„¤ì •ìœ¼ë¡œ ì–‘ìí™”
python quantization.py \
    --checkpoint_path model-checkpoints/kobert/checkpoint-1100 \
    --save_dir quantized-model \
    --test
```

### 2. ì–‘ìí™” ê³¼ì • ìƒì„¸

```
=== í™˜ê²½ ì„¤ì • ===
GPU ì‚¬ìš© ê°€ëŠ¥: NVIDIA GeForce RTX 3080
GPU ë©”ëª¨ë¦¬: 10.0GB
í™˜ê²½ ì„¤ì • ì™„ë£Œ

=== ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ===
ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: skt/kobert-base-v1
ëª¨ë¸ íƒ€ì…: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
íŒŒë¼ë¯¸í„° ìˆ˜: 110,100,480

=== LoRA ëª¨ë¸ ë¡œë“œ ë° ë³‘í•© ===
LoRA ëª¨ë¸ ë¡œë“œ ë° ë³‘í•©: model-checkpoints/kobert/checkpoint-1100
LoRA ëª¨ë¸ ë¡œë“œ ë° ë³‘í•© ì™„ë£Œ
ë³‘í•©ëœ ëª¨ë¸ íƒ€ì…: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
íŒŒë¼ë¯¸í„° ìˆ˜: 110,100,480

=== 4-bit ì–‘ìí™” ì ìš© ===
ì„ì‹œ ëª¨ë¸ ì €ì¥ ì™„ë£Œ
4-bit ì–‘ìí™” ì ìš© ì™„ë£Œ
ì–‘ìí™”ëœ ëª¨ë¸ íƒ€ì…: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>

=== ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥ ===
ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥: bnb-4bit
ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: bnb-4bit

ì €ì¥ëœ íŒŒì¼ë“¤:
- config.json
- pytorch_model.bin
- tokenizer.json
- tokenizer_config.json
- vocab.txt

=== ëª¨ë¸ í¬ê¸° ë¹„êµ ===
ì›ë³¸ ëª¨ë¸ (ì˜ˆìƒ): 420.0 MB
ì–‘ìí™”ëœ ëª¨ë¸: 105.2 MB
í¬ê¸° ê°ì†Œìœ¨: 75.0%

=== ì–‘ìí™”ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===
í…ìŠ¤íŠ¸: ì•ˆë…•í•˜ì„¸ìš”! ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”.
ì˜ˆì¸¡: none (ì‹ ë¢°ë„: 0.923)

í…ìŠ¤íŠ¸: ì´ëŸ° ë§ì€ í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.
ì˜ˆì¸¡: toxic (ì‹ ë¢°ë„: 0.856)

í…ìŠ¤íŠ¸: ì •ë§ ë©‹ì§„ í”„ë¡œì íŠ¸ë„¤ìš”!
ì˜ˆì¸¡: none (ì‹ ë¢°ë„: 0.901)
```

### 3. ì–‘ìí™” íŒŒë¼ë¯¸í„° ì„¤ëª…

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|----------|--------|------|
| `--checkpoint_path` | í•„ìˆ˜ | LoRA ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ |
| `--base_model` | skt/kobert-base-v1 | ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ |
| `--save_dir` | bnb-4bit | ì–‘ìí™”ëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ |
| `--test` | False | ì–‘ìí™” í›„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì—¬ë¶€ |

## ê²°ê³¼ ë¶„ì„

### 1. ëª¨ë¸ í¬ê¸° ë¹„êµ

```bash
# ì›ë³¸ ëª¨ë¸ í¬ê¸° í™•ì¸
du -sh model-checkpoints/kobert/checkpoint-1100/

# ì–‘ìí™”ëœ ëª¨ë¸ í¬ê¸° í™•ì¸
du -sh bnb-4bit/
```

### 2. ì„±ëŠ¥ ë¹„êµ

| ì§€í‘œ | ì›ë³¸ ëª¨ë¸ | ì–‘ìí™”ëœ ëª¨ë¸ | ì°¨ì´ |
|------|-----------|---------------|------|
| ì •í™•ë„ | 88.2% | 87.8% | -0.4% |
| F1 ì ìˆ˜ | 87.6% | 87.2% | -0.4% |
| ì¶”ë¡  ì‹œê°„ | 100ms | 60ms | -40% |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | 2GB | 0.5GB | -75% |

### 3. í’ˆì§ˆ ê²€ì¦

```bash
# ì–‘ìí™”ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
python quantization.py \
    --checkpoint_path model-checkpoints/kobert/checkpoint-1100 \
    --test
```

## ì–‘ìí™”ëœ ëª¨ë¸ ì‚¬ìš©

### 1. ê¸°ë³¸ ì‚¬ìš©ë²•

```python
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
model = AutoModelForSequenceClassification.from_pretrained('bnb-4bit', device_map='auto')
tokenizer = AutoTokenizer.from_pretrained('bnb-4bit')

# ì˜ˆì¸¡ í•¨ìˆ˜
def predict_toxic(text):
    prompt = f"ë‹¤ìŒ ë¬¸ì¥ì´ ê¸ì •ì¸ì§€ ë¶€ì •ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.\n\n### ë¬¸ì¥:\n{text}"
    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=512)
    
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.softmax(outputs.logits, dim=-1)
        predicted_class = torch.argmax(predictions, dim=-1).item()
    
    return "toxic" if predicted_class == 0 else "none"

# ì‚¬ìš© ì˜ˆì‹œ
result = predict_toxic("í…ŒìŠ¤íŠ¸ ë¬¸ì¥")
print(f"ê²°ê³¼: {result}")
```

### 2. ë°°ì¹˜ ì²˜ë¦¬

```python
def predict_batch(texts):
    results = []
    for text in texts:
        result = predict_toxic(text)
        results.append(result)
    return results

# ë°°ì¹˜ ì˜ˆì¸¡
texts = ["ì•ˆë…•í•˜ì„¸ìš”", "ì´ëŸ° ë§ì€ ì•ˆ ë©ë‹ˆë‹¤", "ë©‹ì§„ í”„ë¡œì íŠ¸ë„¤ìš”"]
results = predict_batch(texts)
print(results)
```

### 3. ì›¹ ì„œë¹„ìŠ¤ í†µí•©

```python
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()
    text = data.get('text', '')
    
    result = predict_toxic(text)
    
    return jsonify({
        'text': text,
        'prediction': result,
        'model': 'quantized-korean-toxic-classifier'
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

## ë¬¸ì œ í•´ê²°

### 1. CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ**: `CUDA out of memory` ì˜¤ë¥˜

**í•´ê²°ë°©ë²•**:
```bash
# CPUë¡œ ì‹¤í–‰
python quantization.py --checkpoint_path model-checkpoints/kobert/checkpoint-1100 --device cpu

# ë” ì‘ì€ ë°°ì¹˜ë¡œ ì²˜ë¦¬
export CUDA_VISIBLE_DEVICES=0
```

### 2. ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ

**ì¦ìƒ**: `FileNotFoundError: ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤`

**í•´ê²°ë°©ë²•**:
```bash
# ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ í™•ì¸
ls -la model-checkpoints/kobert/

# ì˜¬ë°”ë¥¸ ê²½ë¡œ ì§€ì •
python quantization.py --checkpoint_path model-checkpoints/kobert/checkpoint-1100
```

### 3. ì–‘ìí™” í›„ ì„±ëŠ¥ ì €í•˜

**ì¦ìƒ**: ì •í™•ë„ê°€ í¬ê²Œ ë–¨ì–´ì§

**í•´ê²°ë°©ë²•**:
```bash
# ë‹¤ë¥¸ ì²´í¬í¬ì¸íŠ¸ ì‹œë„
python quantization.py --checkpoint_path model-checkpoints/kobert/checkpoint-1000

# 8-bit ì–‘ìí™” ì‹œë„ (ë” ë†’ì€ ì •í™•ë„)
# quantization.py ìˆ˜ì •í•˜ì—¬ load_in_8bit=True ì‚¬ìš©
```

### 4. bitsandbytes ì„¤ì¹˜ ì˜¤ë¥˜

**í•´ê²°ë°©ë²•**:
```bash
# CUDA ë²„ì „ í™•ì¸
nvidia-smi

# ë§ëŠ” ë²„ì „ ì„¤ì¹˜
pip install bitsandbytes --upgrade

# ë˜ëŠ” ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜
pip install git+https://github.com/TimDettmers/bitsandbytes.git
```

## ê³ ê¸‰ ì„¤ì •

### 1. ë‹¤ì–‘í•œ ì–‘ìí™” ë°©ë²•

```python
# 8-bit ì–‘ìí™” (ë” ë†’ì€ ì •í™•ë„)
model = AutoModelForSequenceClassification.from_pretrained(
    model_path, 
    load_in_8bit=True, 
    device_map="auto"
)

# ë™ì  ì–‘ìí™” (PyTorch ë‚´ì¥)
import torch.quantization
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
```

### 2. ì–‘ìí™” í’ˆì§ˆ ìµœì í™”

```python
# ì–‘ìí™” ìº˜ë¦¬ë¸Œë ˆì´ì…˜
def calibrate_model(model, calibration_data):
    model.eval()
    with torch.no_grad():
        for batch in calibration_data:
            model(batch)
    return model

# ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í›„ ì–‘ìí™”
calibrated_model = calibrate_model(model, calibration_loader)
```

### 3. ì–‘ìí™” ì„¤ì • íŠœë‹

```python
# ì–‘ìí™” ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

model = AutoModelForSequenceClassification.from_pretrained(
    model_path,
    quantization_config=quantization_config,
    device_map="auto"
)
```

## ì„±ëŠ¥ ìµœì í™” íŒ

### 1. ì–‘ìí™” ì „ ìµœì í™”
- ëª¨ë¸ ì••ì¶• (Pruning) ì ìš©
- ì§€ì‹ ì¦ë¥˜ (Knowledge Distillation) ì‚¬ìš©
- ë” ì‘ì€ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„ íƒ

### 2. ì–‘ìí™” í›„ ìµœì í™”
- ë°°ì¹˜ í¬ê¸° ì¡°ì •
- ì¶”ë¡  ì—”ì§„ ìµœì í™” (TensorRT, ONNX)
- ìºì‹± ì „ëµ ì ìš©

### 3. ëª¨ë‹ˆí„°ë§ ë° ìœ ì§€ë³´ìˆ˜
- ì •ê¸°ì ì¸ ì„±ëŠ¥ ì¸¡ì •
- ë“œë¦¬í”„íŠ¸ ê°ì§€
- ëª¨ë¸ ì—…ë°ì´íŠ¸ ê³„íš

## ë‹¤ìŒ ë‹¨ê³„

ì–‘ìí™”ê°€ ì™„ë£Œë˜ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì§„í–‰í•˜ì„¸ìš”:

1. **[ì‚¬ìš©ë²• ì˜ˆì‹œ](USAGE_EXAMPLES.md)**: ì–‘ìí™”ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡í•˜ê¸°
2. **[ê²°ê³¼ ë¶„ì„](results/README.md)**: ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„

## ê´€ë ¨ ê°€ì´ë“œ

- **[ë¹ ë¥¸ ì‹œì‘](QUICK_START.md)**: 5ë¶„ ë§Œì— í”„ë¡œì íŠ¸ ì‹¤í–‰í•˜ê¸°
- **[í•™ìŠµ ê°€ì´ë“œ](TRAINING_GUIDE.md)**: ëª¨ë¸ í•™ìŠµ ê³¼ì • ìƒì„¸ ì„¤ëª…
- **[ì‚¬ìš©ë²• ì˜ˆì‹œ](USAGE_EXAMPLES.md)**: ë‹¤ì–‘í•œ í™œìš© ë°©ë²•

## ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸

ì–‘ìí™” ì™„ë£Œ í›„ í™•ì¸ì‚¬í•­:

- [ ] ëª¨ë¸ í¬ê¸°ê°€ 75% ì´ìƒ ê°ì†Œí–ˆëŠ”ê°€?
- [ ] ì •í™•ë„ ì†ì‹¤ì´ 1% ì´í•˜ì¸ê°€?
- [ ] ì¶”ë¡  ì†ë„ê°€ í–¥ìƒë˜ì—ˆëŠ”ê°€?
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ê°ì†Œí–ˆëŠ”ê°€?
- [ ] ì–‘ìí™”ëœ ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ëŠ”ê°€?
- [ ] ì˜ˆì¸¡ ê²°ê³¼ê°€ ì›ë³¸ ëª¨ë¸ê³¼ ì¼ì¹˜í•˜ëŠ”ê°€?

---

**ì°¸ê³ **: ì–‘ìí™”ëŠ” ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ì•½ê°„ ë–¨ì–´ëœ¨ë¦´ ìˆ˜ ìˆì§€ë§Œ, ë©”ëª¨ë¦¬ì™€ ì†ë„ ì¸¡ë©´ì—ì„œ í° ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤. ì‹¤ì œ ì‚¬ìš© í™˜ê²½ì— ë§ëŠ” ì ì ˆí•œ ê· í˜•ì ì„ ì°¾ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. 