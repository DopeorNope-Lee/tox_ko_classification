# í•œêµ­ì–´ ì•…ì„± ëŒ“ê¸€ ë¶„ë¥˜ ëª¨ë¸ (Korean Toxic Comment Classification)

> **KoBERT + LoRA fineâ€‘tuning, 4â€‘bit quantization, and readyâ€‘toâ€‘use CLI tools**
> Detect toxic comments in Korean text with lightweight, productionâ€‘ready models.

<p align="center">
  <img src="https://img.shields.io/badge/python-3.11.8%2B-blue" />
  <img src="https://img.shields.io/badge/torch-2.6%2B-ff69b4" />
  <img src="https://img.shields.io/badge/transformers-4.53.0%2B-yellow" />
  <img src="https://img.shields.io/badge/license-MIT-green" />
</p>

---

## ì£¼ìš” íŠ¹ì§•

* **ê²½ëŸ‰ ëª¨ë¸**Â â€“ KoBERTì— LoRAë¥¼ ì ìš©í•´Â ğŸ’¾Â ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ **75â€¯% ì´ìƒ** ì ˆê°í•˜ê³ ë„ 88â€¯%+ ì •í™•ë„ ìœ ì§€.
* **4â€‘bit ì–‘ìí™”**Â â€“Â `bitsandbytes`Â ì§€ì› GPUì—ì„œ ì‹¤ì‹œê°„ ì¶”ë¡ ì´ ê°€ëŠ¥í•˜ë„ë¡ ëª¨ë¸ì„ 4â€‘bitë¡œ ë³€í™˜.Â ğŸª„
* **ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ CLI**Â â€“ `train.py`, `quantization.py`, `inference.py` ìŠ¤í¬ë¦½íŠ¸ ì œê³µ.
* **ëª¨ë“ˆí™”ëœ ì½”ë“œë² ì´ìŠ¤**Â â€“Â `utils/`Â ì— ë°ì´í„° ë¡œë”©Â·ëª¨ë¸ë§Â·í‰ê°€ì§€í‘œ í•¨ìˆ˜ ë¶„ë¦¬.
* **ìì„¸í•œ ê°€ì´ë“œ**Â â€“ ë¹ ë¥¸ ì‹œì‘, í•™ìŠµ, ì–‘ìí™”, ì˜ˆì‹œ ë¬¸ì„œë¥¼ ë³„ë„Â `*.md`Â íŒŒì¼ë¡œ ì œê³µ.

---

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```text
 tox_ko_classification/
 â”œâ”€â”€ train.py                  # ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
 â”œâ”€â”€ quantization.py           # 4â€‘bit ì–‘ìí™” ìŠ¤í¬ë¦½íŠ¸
 â”œâ”€â”€ inference.py              # ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ (ë‹¨ì¼Â·ë°°ì¹˜Â·ëŒ€í™”í˜•)
 â”œâ”€â”€ requirements.txt          # ì˜ì¡´ íŒ¨í‚¤ì§€ ë²„ì „ ëª…ì‹œ
 â”œâ”€â”€ setup.py                  # íŒ¨í‚¤ì§€ ë° í™˜ê²½ ì„¤ì • ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
 â”œâ”€â”€ utils/                    # ë°ì´í„°/ëª¨ë¸ ìœ í‹¸ë¦¬í‹°
 â”‚   â”œâ”€â”€ data.py               # ë°ì´í„°ì…‹ ë¡œë”©Â·í† í°í™”
 â”‚   â”œâ”€â”€ modeling.py           # LoRA ëª¨ë¸ ë¹Œë“œ
 â”‚   â””â”€â”€ â€¦                     # collator, metric ë“±
 â”œâ”€â”€ data/                     # í•™ìŠµ ë°ì´í„° (10â€¯k ìƒ˜í”Œ)
 â”‚   â””â”€â”€ README.md             # ë°ì´í„°ì…‹ ìƒì„¸ ì„¤ëª…
 â”œâ”€â”€ examples/                 # ì…ë ¥ ì˜ˆì‹œ
 â”œâ”€â”€ results/                  # í•™ìŠµ ê²°ê³¼ ë° ì²´í¬í¬ì¸íŠ¸
 â””â”€â”€ docs/                     # ë¬¸ì„œ(ë¹ ë¥¸ ì‹œì‘ ë“±) â€“ ì„ íƒ
```

---

## ë¹ ë¥¸ ì‹œì‘

```bash
# 1ï¸âƒ£ ì €ì¥ì†Œ í´ë¡  & ì˜ì¡´ì„± ì„¤ì¹˜
$ git clone https://github.com/DopeorNope-Lee/tox_ko_classification.git
$ cd tox_ko_classification
$ python setup.py

# 2ï¸âƒ£ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ (GPU ê¶Œì¥)
$ python train.py  # ì•½ 30ë¶„â€“2ì‹œê°„

# 3ï¸âƒ£ 4â€‘bit ì–‘ìí™” (ì„ íƒ)
$ python quantization.py 

# 4ï¸âƒ£ ë‹¨ì¼ ë¬¸ì¥ ì¶”ë¡ 
$ python inference.py
```

---

## ğŸ‹ï¸â€â™€ï¸ í•™ìŠµ ì˜µì…˜

`train.py` ì˜ ê¸°ë³¸ ì„¤ì •ì€ ìŠ¤í¬ë¦½íŠ¸ ìƒë‹¨ì˜ `CONFIG` ë”•ì…”ë„ˆë¦¬ë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤.
ì˜ˆì‹œ:

```bash
python train.py \
```

| ì¸ì             | ê¸°ë³¸ê°’                       | ì„¤ëª…            |
| -------------- | ------------------------- | ------------- |
| `model_name` | `skt/kobert-base-v1`      | ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ |
| `epochs`     | `5`                       | í•™ìŠµ epoch ìˆ˜    |
| `batch_size` | `32`                      | GPU ë‹¹ ë°°ì¹˜ í¬ê¸°   |
| `lr`         | `2e-5`                    | í•™ìŠµë¥            |
| `output_dir` | `checkpoints/kobert-lora` | ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ   |

í•™ìŠµì´ ì™„ë£Œë˜ë©´ ê°€ì¥ ë‚®ì€ `eval_loss` ë¥¼ ê¸°ë¡í•œ ëª¨ë¸ì´ `output_dir`ì— ì €ì¥ë©ë‹ˆë‹¤.

---

## 4â€‘bit ì–‘ìí™”

`quantization.py` ëŠ” LoRA ê°€ì¤‘ì¹˜ë¥¼ Merge í•œ ë’¤ `bitsandbytes` ì˜ 4â€‘bit ì–‘ìí™” ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— ì•„ë˜ `quantization.py` ë‚´ë¶€ configì— í•™ìŠµ í›„ ì €ì¥ë¼ìˆëŠ” `lora_dir`ì„ ë„£ì–´ì£¼ì„¸ìš”!

```
CONFIG = {
    "base_model": "skt/kobert-base-v1",
    "lora_dir":   "checkpoints/kobert-lora/checkpoint-700", # ì´ ë¶€ë¶„ì„ í˜„ì¬ ìˆëŠ” checkpointë¡œ ìˆ˜ì •í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    "save_dir":   "checkpoints/kobert-bnb-4bit",
}
```

```bash
python quantization.py \

```

ìƒì„±ëœ ë””ë ‰í„°ë¦¬ë¥¼ `inference.py ì˜ model` ì¸ìë¡œ ë„˜ê¸°ë©´ GPU ë©”ëª¨ë¦¬ \~2â€¯GB ìˆ˜ì¤€ì—ì„œë„ ì¶”ë¡ ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

---

## ğŸ” ì¶”ë¡  ì‚¬ìš©ë²•

ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— ì•„ë˜ `inference.py` ë‚´ë¶€ configì— í•™ìŠµ í›„ ì €ì¥ë¼ìˆëŠ” `lora_dir`ì„ ë„£ì–´ì£¼ì„¸ìš”!

```
CONFIG = {
    "base_model": "skt/kobert-base-v1",
    "lora_dir": "checkpoints/kobert-lora/checkpoint-700", # ì´ ë¶€ë¶„ì„ í˜„ì¬ ìˆëŠ” checkpointë¡œ ìˆ˜ì •í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
}
```

text, file, interactive ëª¨ë“œ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ì¶”ë¡ ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
- text: ë¶„ë¥˜í•  ë‹¨ì¼ í…ìŠ¤íŠ¸
- file: ë¶„ë¥˜í•  í…ìŠ¤íŠ¸ê°€ ë‹´ê¸´ íŒŒì¼ ê²½ë¡œ (í•œ ì¤„ì— í•œ í…ìŠ¤íŠ¸)
- interactive: ëŒ€í™”í˜• ëª¨ë“œë¡œ ì‹¤í–‰

```bash

python inference.py --text "ë„ˆë¬´ ì¬ë°Œê²Œ ë´¤ìŠµë‹ˆë‹¤!"
```

---

## ì„±ëŠ¥ ìš”ì•½ (dev set 500ê°œ)

| ëª¨ë¸                    | íŒŒë¼ë¯¸í„°     | ì–‘ìí™” | Accuracy   | F1    | VRAM(â†˜)    |
| --------------------- | -------- | --- | ---------- | ----- | ---------- |
| KoBERT (baseline)     | 110â€¯M    | âŒ   | 90.6â€¯%     | 0.901 | 6.5â€¯GB     |
| **KoBERT + LoRA**     | 35â€¯M (Î”) | âŒ   | **88.2â€¯%** | 0.876 | 2.4â€¯GB     |
| **KoBERT LoRA 4â€‘bit** | 35â€¯M     | âœ…   | 87.9â€¯%     | 0.872 | **1.6â€¯GB** |

> *ì¸¡ì • í™˜ê²½: RTXÂ 3090, PyTorchÂ 2.1.0, FP16*
> ìì„¸í•œ ë²¤ì¹˜ë§ˆí¬ëŠ” [`results/`](results/) í´ë” ì°¸ì¡°.

---

## ë°ì´í„°ì…‹

* **ê·œëª¨**Â : 10â€¯000 ë¬¸ì¥ (toxicÂ 5â€¯000 / noneÂ 5â€¯000)
* **ì¶œì²˜**Â : Korean Hate Speech Dataset, Curse Detection Dataset, ìì²´ ë¼ë²¨ë§
* **ë¼ì´ì„ ìŠ¤**Â : ì—°êµ¬Â·êµìœ¡ ëª©ì  ì‚¬ìš©ì— í•œí•¨ (ìƒì—…ì  ì´ìš© ì‹œ ì›ì €ì‘ì í˜‘ì˜ í•„ìš”)

ë°ì´í„° ì „ì²˜ë¦¬ ìƒì„¸ ê³¼ì •ì€ [`data/README.md`](data/README.md) ì°¸ê³ .

---

## ìš”êµ¬ ì‚¬í•­

ì˜ì¡´ì„± ì¼ê´„ ì„¤ì¹˜:

```bash
python setup.py
# or
pip install -r requirements.txt  # torchëŠ” CUDA í™˜ê²½ì— ë§ì¶° ìˆ˜ë™ ì„¤ì¹˜ ê°€ëŠ¥
```

---

## ì°¸ê³  ë¬¸í—Œ

* **KoBERT** â€“ SKTBrain.
* **PEFT: Parameterâ€‘Efficient Fineâ€‘Tuning** â€“ Hugging Face.
* **bitsandbytes** â€“ Tim Dettmers.

---

<p align="center">Made with â¤ï¸Â by DopeorNopeâ€‘Lee </p>
