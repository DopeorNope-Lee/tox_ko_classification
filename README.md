# í•œêµ­ì–´ ì•…ì„± ëŒ“ê¸€ ë¶„ë¥˜ ëª¨ë¸ (Korean Toxic Comment Classification)

> **KoBERT + LoRA fineâ€‘tuning, 4â€‘bit quantization, and readyâ€‘toâ€‘use CLI tools**
> Detect toxic comments in Korean text with lightweight, productionâ€‘ready models.

<p align="center">
  <img src="https://img.shields.io/badge/python-3.9%2B-blue" />
  <img src="https://img.shields.io/badge/torch-2.1%2B-ff69b4" />
  <img src="https://img.shields.io/badge/transformers-4.35%2B-yellow" />
  <img src="https://img.shields.io/badge/license-MIT-green" />
</p>

---

## âœ¨ ì£¼ìš” íŠ¹ì§•

* **ê²½ëŸ‰ ëª¨ë¸**Â â€“ KoBERTì— LoRAë¥¼ ì ìš©í•´Â ğŸ’¾Â ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ **75â€¯% ì´ìƒ** ì ˆê°í•˜ê³ ë„ 88â€¯%+ ì •í™•ë„ ìœ ì§€.
* **4â€‘bit ì–‘ìí™”**Â â€“Â `bitsandbytes`Â ì§€ì› GPUì—ì„œ ì‹¤ì‹œê°„ ì¶”ë¡ ì´ ê°€ëŠ¥í•˜ë„ë¡ ëª¨ë¸ì„ 4â€‘bitë¡œ ë³€í™˜.Â ğŸª„
* **ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ CLI**Â â€“ `train.py`, `quantization.py`, `inference.py` ìŠ¤í¬ë¦½íŠ¸ ì œê³µ.
* **ëª¨ë“ˆí™”ëœ ì½”ë“œë² ì´ìŠ¤**Â â€“Â `utils/`Â ì— ë°ì´í„° ë¡œë”©Â·ëª¨ë¸ë§Â·í‰ê°€ì§€í‘œ í•¨ìˆ˜ ë¶„ë¦¬.
* **ìì„¸í•œ ê°€ì´ë“œ**Â â€“ ë¹ ë¥¸ ì‹œì‘, í•™ìŠµ, ì–‘ìí™”, ì˜ˆì‹œ ë¬¸ì„œë¥¼ ë³„ë„Â `*.md`Â íŒŒì¼ë¡œ ì œê³µ.

---

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

```text
 tox_ko_classification/
 â”œâ”€â”€ train.py                  # ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
 â”œâ”€â”€ quantization.py           # 4â€‘bit ì–‘ìí™” ìŠ¤í¬ë¦½íŠ¸
 â”œâ”€â”€ inference.py              # ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ (ë‹¨ì¼Â·ë°°ì¹˜Â·ëŒ€í™”í˜•)
 â”œâ”€â”€ requirements.txt          # ì˜ì¡´ íŒ¨í‚¤ì§€ ë²„ì „ ëª…ì‹œ
 â”œâ”€â”€ utils/                    # ë°ì´í„°/ëª¨ë¸ ìœ í‹¸ë¦¬í‹°
 â”‚   â”œâ”€â”€ data.py               # ë°ì´í„°ì…‹ ë¡œë”©Â·í† í°í™”
 â”‚   â”œâ”€â”€ modeling.py           # LoRA ëª¨ë¸ ë¹Œë“œ
 â”‚   â””â”€â”€ â€¦                     # collator, metric ë“±
 â”œâ”€â”€ data/                     # í•™ìŠµ ë°ì´í„° (10â€¯k ìƒ˜í”Œ)
 â”‚   â””â”€â”€ README.md             # ë°ì´í„°ì…‹ ìƒì„¸ ì„¤ëª…
 â”œâ”€â”€ examples/                 # ì…ë ¥ ì˜ˆì‹œ
 â”œâ”€â”€ results/                  # í•™ìŠµ ê²°ê³¼ ë° ì²´í¬í¬ì¸íŠ¸
 â””â”€â”€ docs/                     # ë¬¸ì„œ(ë¹ ë¥¸ ì‹œì‘ ë“±) â€“ ì„ íƒ
```

---

## âš¡ï¸ ë¹ ë¥¸ ì‹œì‘ (5ë¶„ ì»·)

```bash
# 1ï¸âƒ£ ì €ì¥ì†Œ í´ë¡  & ì˜ì¡´ì„± ì„¤ì¹˜
$ git clone https://github.com/DopeorNope-Lee/tox_ko_classification.git
$ cd tox_ko_classification
$ python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate
$ pip install -r requirements.txt

# 2ï¸âƒ£ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ (GPU ê¶Œì¥)
$ python train.py  # ì•½ 30ë¶„â€“2ì‹œê°„

# 3ï¸âƒ£ 4â€‘bit ì–‘ìí™” (ì„ íƒ)
$ python quantization.py \
    --base_model checkpoints/kobert-lora/checkpoint-700 \
    --save_dir checkpoints/kobert-bnb-4bit

# 4ï¸âƒ£ ë‹¨ì¼ ë¬¸ì¥ ì¶”ë¡ 
$ python inference.py --model checkpoints/kobert-bnb-4bit \
    --text "ë„ˆ ì •ë§ ëª»ëë‹¤!"
```

---

## ğŸ‹ï¸â€â™€ï¸ í•™ìŠµ ì˜µì…˜

`train.py` ì˜ ê¸°ë³¸ ì„¤ì •ì€ ìŠ¤í¬ë¦½íŠ¸ ìƒë‹¨ì˜ `CONFIG` ë”•ì…”ë„ˆë¦¬ë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤. CLI ì¸ìë¥¼ í†µí•´ ë®ì–´ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì˜ˆì‹œ:

```bash
python train.py \
  --epochs 10 \
  --batch_size 64 \
  --lr 3e-5 \
  --csv_file custom.csv
```

| ì¸ì             | ê¸°ë³¸ê°’                       | ì„¤ëª…            |
| -------------- | ------------------------- | ------------- |
| `--model_name` | `skt/kobert-base-v1`      | ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ |
| `--epochs`     | `5`                       | í•™ìŠµ epoch ìˆ˜    |
| `--batch_size` | `32`                      | GPU ë‹¹ ë°°ì¹˜ í¬ê¸°   |
| `--lr`         | `2e-5`                    | í•™ìŠµë¥            |
| `--output_dir` | `checkpoints/kobert-lora` | ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ   |

í•™ìŠµì´ ì™„ë£Œë˜ë©´ ê°€ì¥ ë‚®ì€ `eval_loss` ë¥¼ ê¸°ë¡í•œ ëª¨ë¸ì´ `output_dir`ì— ì €ì¥ë©ë‹ˆë‹¤.

---

## ğŸ”® 4â€‘bit ì–‘ìí™”

`quantization.py` ëŠ” LoRA ê°€ì¤‘ì¹˜ë¥¼ Merge í•œ ë’¤ `bitsandbytes` ì˜ 4â€‘bit ì–‘ìí™” ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

```bash
python quantization.py \
  --lora_dir checkpoints/kobert-lora/checkpoint-700 \
  --save_dir checkpoints/kobert-bnb-4bit
```

ìƒì„±ëœ ë””ë ‰í„°ë¦¬ë¥¼ `inference.py --model` ì¸ìë¡œ ë„˜ê¸°ë©´ GPU ë©”ëª¨ë¦¬ \~2â€¯GB ìˆ˜ì¤€ì—ì„œë„ ì¶”ë¡ ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

---

## ğŸ” ì¶”ë¡  ì‚¬ìš©ë²•

```bash
# ë‹¨ì¼ í…ìŠ¤íŠ¸
python inference.py --model checkpoints/kobert-bnb-4bit \
  --text "ì•ˆë…•? ì´ ë©ì²­ì•„!"  # â†’ toxic (conf 0.97)

# í…ìŠ¤íŠ¸ íŒŒì¼ ë°°ì¹˜ ì˜ˆì¸¡
python inference.py --file examples/test_texts.txt --output batch.json

# ëŒ€í™”í˜• ëª¨ë“œ
python inference.py --interactive
```

`inference.py` ëŠ” Softmax ì ìˆ˜ ê¸°ë°˜ ì‹ ë¢°ë„(`confidence`)ë¥¼ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.

---

## ğŸ“Š ì„±ëŠ¥ ìš”ì•½ (dev set 500ê°œ)

| ëª¨ë¸                    | íŒŒë¼ë¯¸í„°     | ì–‘ìí™” | Accuracy   | F1    | VRAM(â†˜)    |
| --------------------- | -------- | --- | ---------- | ----- | ---------- |
| KoBERT (baseline)     | 110â€¯M    | âŒ   | 90.6â€¯%     | 0.901 | 6.5â€¯GB     |
| **KoBERT + LoRA**     | 35â€¯M (Î”) | âŒ   | **88.2â€¯%** | 0.876 | 2.4â€¯GB     |
| **KoBERT LoRA 4â€‘bit** | 35â€¯M     | âœ…   | 87.9â€¯%     | 0.872 | **1.6â€¯GB** |

> *ì¸¡ì • í™˜ê²½: RTXÂ 3090, PyTorchÂ 2.1.0, FP16*
> ìì„¸í•œ ë²¤ì¹˜ë§ˆí¬ëŠ” [`results/`](results/) í´ë” ì°¸ì¡°.

---

## ğŸ—‚ï¸ ë°ì´í„°ì…‹

* **ê·œëª¨**Â : 10â€¯000 ë¬¸ì¥ (toxicÂ 5â€¯000 / noneÂ 5â€¯000)
* **ì¶œì²˜**Â : Korean Hate Speech Dataset, Curse Detection Dataset, ìì²´ ë¼ë²¨ë§
* **ë¼ì´ì„ ìŠ¤**Â : ì—°êµ¬Â·êµìœ¡ ëª©ì  ì‚¬ìš©ì— í•œí•¨ (ìƒì—…ì  ì´ìš© ì‹œ ì›ì €ì‘ì í˜‘ì˜ í•„ìš”)

ë°ì´í„° ì „ì²˜ë¦¬ ìƒì„¸ ê³¼ì •ì€ [`data/README.md`](data/README.md) ì°¸ê³ .

---

## ğŸ’» ìš”êµ¬ ì‚¬í•­

ì˜ì¡´ì„± ì¼ê´„ ì„¤ì¹˜:

```bash
pip install -r requirements.txt  # torchëŠ” CUDA í™˜ê²½ì— ë§ì¶° ìˆ˜ë™ ì„¤ì¹˜ ê°€ëŠ¥
```

---

## ğŸ™ ì°¸ê³  ë¬¸í—Œ

* **KoBERT** â€“ SKTBrain.
* **PEFT: Parameterâ€‘Efficient Fineâ€‘Tuning** â€“ Hugging Face.
* **bitsandbytes** â€“ Tim Dettmers.

---

<p align="center">Made with â¤ï¸Â by DopeorNopeâ€‘Lee & Contributors</p>
