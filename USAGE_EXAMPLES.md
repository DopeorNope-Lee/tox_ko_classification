# ğŸš€ ì‚¬ìš©ë²• ì˜ˆì‹œ (Usage Examples)

ì´ ë¬¸ì„œëŠ” í•œêµ­ì–´ ì•…ì„± ëŒ“ê¸€ ë¶„ë¥˜ í”„ë¡œì íŠ¸ì˜ ë‹¤ì–‘í•œ ì‚¬ìš©ë²•ì„ ì˜ˆì‹œì™€ í•¨ê»˜ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [ë¹ ë¥¸ ì‹œì‘](#ë¹ ë¥¸-ì‹œì‘)
2. [ëª¨ë¸ í•™ìŠµ](#ëª¨ë¸-í•™ìŠµ)
3. [ëª¨ë¸ ì–‘ìí™”](#ëª¨ë¸-ì–‘ìí™”)
4. [ì¶”ë¡  ë° ì˜ˆì¸¡](#ì¶”ë¡ -ë°-ì˜ˆì¸¡)
5. [ì›¹ ì„œë¹„ìŠ¤](#ì›¹-ì„œë¹„ìŠ¤)
6. [ë°°ì¹˜ ì²˜ë¦¬](#ë°°ì¹˜-ì²˜ë¦¬)

## âš¡ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone <repository-url>
cd tox_ko_classification

# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv tox_env
source tox_env/bin/activate  # Linux/Mac
# ë˜ëŠ”
tox_env\Scripts\activate     # Windows

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 2. ê¸°ë³¸ ì˜ˆì¸¡ (ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©)

```python
from inference import ToxicClassifier

# ë¶„ë¥˜ê¸° ì´ˆê¸°í™”
classifier = ToxicClassifier()

# ë‹¨ì¼ í…ìŠ¤íŠ¸ ì˜ˆì¸¡
text = "ì•ˆë…•í•˜ì„¸ìš”! ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”."
result = classifier.predict(text)
print(f"í…ìŠ¤íŠ¸: {text}")
print(f"ì˜ˆì¸¡: {result['prediction']}")
print(f"ì‹ ë¢°ë„: {result['confidence']:.3f}")
```

## ğŸ“ ëª¨ë¸ í•™ìŠµ

### 1. ê¸°ë³¸ í•™ìŠµ

```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
python train.py

# ì»¤ìŠ¤í…€ íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµ
python train.py \
    --epochs 20 \
    --batch_size 128 \
    --learning_rate 5e-5 \
    --output_dir model-checkpoints/kobert
```

### 2. í•™ìŠµ ê³¼ì • ëª¨ë‹ˆí„°ë§

```bash
# í•™ìŠµ ë¡œê·¸ í™•ì¸
tail -f model-checkpoints/kobert/trainer_state.json

# GPU ì‚¬ìš©ëŸ‰ í™•ì¸
watch -n 1 nvidia-smi
```

### 3. í•™ìŠµ ê²°ê³¼ í™•ì¸

```python
import json

# í•™ìŠµ ê²°ê³¼ ë¡œë“œ
with open('model-checkpoints/kobert/trainer_state.json', 'r') as f:
    trainer_state = json.load(f)

# ìµœê³  ì„±ëŠ¥ í™•ì¸
best_metric = trainer_state['best_metric']
print(f"ìµœê³  ì •í™•ë„: {best_metric:.4f}")
```

## âš¡ ëª¨ë¸ ì–‘ìí™”

### 1. ê¸°ë³¸ ì–‘ìí™”

```bash
# í•™ìŠµëœ ëª¨ë¸ ì–‘ìí™”
python quantization.py \
    --checkpoint_path model-checkpoints/kobert/checkpoint-1100 \
    --save_dir quantized-model \
    --test
```

### 2. ì–‘ìí™” ê²°ê³¼ í™•ì¸

```python
import os

# ëª¨ë¸ í¬ê¸° ë¹„êµ
original_size = os.path.getsize('model-checkpoints/kobert/checkpoint-1100/pytorch_model.bin')
quantized_size = os.path.getsize('quantized-model/pytorch_model.bin')

reduction = (original_size - quantized_size) / original_size * 100
print(f"ëª¨ë¸ í¬ê¸° ê°ì†Œìœ¨: {reduction:.1f}%")
```

### 3. ì–‘ìí™”ëœ ëª¨ë¸ ì‚¬ìš©

```python
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
model = AutoModelForSequenceClassification.from_pretrained('quantized-model', device_map='auto')
tokenizer = AutoTokenizer.from_pretrained('quantized-model')

# ì˜ˆì¸¡
def predict_quantized(text):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.softmax(outputs.logits, dim=-1)
        predicted_class = torch.argmax(predictions, dim=-1).item()
    return "toxic" if predicted_class == 0 else "none"

result = predict_quantized("í…ŒìŠ¤íŠ¸ ë¬¸ì¥")
print(f"ê²°ê³¼: {result}")
```

## ğŸ” ì¶”ë¡  ë° ì˜ˆì¸¡

### 1. ë‹¨ì¼ í…ìŠ¤íŠ¸ ì˜ˆì¸¡

```python
from inference import ToxicClassifier

classifier = ToxicClassifier()

# ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ì˜ˆì¸¡
test_texts = [
    "ì•ˆë…•í•˜ì„¸ìš”! ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”.",
    "ì´ëŸ° ë§ì€ í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.",
    "ì •ë§ ë©‹ì§„ í”„ë¡œì íŠ¸ë„¤ìš”!",
    "ë„ˆëŠ” ì •ë§ ë°”ë³´ì•¼.",
    "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”."
]

for text in test_texts:
    result = classifier.predict(text)
    print(f"í…ìŠ¤íŠ¸: {text}")
    print(f"ì˜ˆì¸¡: {result['prediction']} (ì‹ ë¢°ë„: {result['confidence']:.3f})")
    print("-" * 50)
```

### 2. ë°°ì¹˜ ì˜ˆì¸¡

```python
# ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì˜ˆì¸¡
texts = ["í…ìŠ¤íŠ¸1", "í…ìŠ¤íŠ¸2", "í…ìŠ¤íŠ¸3", ...]
results = classifier.predict_batch(texts)

for text, result in zip(texts, results):
    print(f"{text}: {result['prediction']}")
```

### 3. ì‹ ë¢°ë„ ì„ê³„ê°’ ì„¤ì •

```python
# ë†’ì€ ì‹ ë¢°ë„ë§Œ ì‹ ë¢°
def predict_with_threshold(text, threshold=0.8):
    result = classifier.predict(text)
    if result['confidence'] >= threshold:
        return result['prediction']
    else:
        return "uncertain"

# ì‚¬ìš© ì˜ˆì‹œ
result = predict_with_threshold("ëª¨í˜¸í•œ í…ìŠ¤íŠ¸", threshold=0.9)
print(f"ê²°ê³¼: {result}")
```

## ì›¹ ì„œë¹„ìŠ¤

### 1. Flask ì›¹ ì„œë²„

```python
from flask import Flask, request, jsonify
from inference import ToxicClassifier

app = Flask(__name__)
classifier = ToxicClassifier()

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()
    text = data.get('text', '')
    
    if not text:
        return jsonify({'error': 'í…ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400
    
    result = classifier.predict(text)
    
    return jsonify({
        'text': text,
        'prediction': result['prediction'],
        'confidence': result['confidence'],
        'model': 'korean-toxic-classifier'
    })

@app.route('/health', methods=['GET'])
def health():
    return jsonify({'status': 'healthy'})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False)
```

### 2. API ì‚¬ìš© ì˜ˆì‹œ

```python
import requests

# API í˜¸ì¶œ
url = "http://localhost:5000/predict"
data = {"text": "í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤."}

response = requests.post(url, json=data)
result = response.json()

print(f"ì˜ˆì¸¡ ê²°ê³¼: {result}")
```

### 3. cURL ì‚¬ìš©

```bash
# API í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "ì•ˆë…•í•˜ì„¸ìš”! ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”."}'
```

## ë°°ì¹˜ ì²˜ë¦¬

### 1. íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì½ê¸°

```python
import pandas as pd
from inference import ToxicClassifier

# CSV íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì½ê¸°
df = pd.read_csv('input_texts.csv')
classifier = ToxicClassifier()

# ë°°ì¹˜ ì˜ˆì¸¡
results = []
for text in df['text']:
    result = classifier.predict(text)
    results.append(result)

# ê²°ê³¼ë¥¼ DataFrameì— ì¶”ê°€
df['prediction'] = [r['prediction'] for r in results]
df['confidence'] = [r['confidence'] for r in results]

# ê²°ê³¼ ì €ì¥
df.to_csv('output_results.csv', index=False)
```

### 2. ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬

```python
import pandas as pd
from tqdm import tqdm
from inference import ToxicClassifier

def process_large_file(input_file, output_file, batch_size=1000):
    classifier = ToxicClassifier()
    
    # ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    chunks = pd.read_csv(input_file, chunksize=batch_size)
    
    for i, chunk in enumerate(chunks):
        print(f"ì²˜ë¦¬ ì¤‘: ì²­í¬ {i+1}")
        
        results = []
        for text in tqdm(chunk['text']):
            result = classifier.predict(text)
            results.append(result)
        
        # ê²°ê³¼ ì¶”ê°€
        chunk['prediction'] = [r['prediction'] for r in results]
        chunk['confidence'] = [r['confidence'] for r in results]
        
        # íŒŒì¼ì— ì¶”ê°€
        mode = 'w' if i == 0 else 'a'
        header = i == 0
        chunk.to_csv(output_file, mode=mode, header=header, index=False)

# ì‚¬ìš© ì˜ˆì‹œ
process_large_file('large_input.csv', 'large_output.csv')
```

### 3. ë©€í‹°í”„ë¡œì„¸ì‹±

```python
import multiprocessing as mp
from functools import partial
from inference import ToxicClassifier

def predict_worker(text, classifier):
    return classifier.predict(text)

def process_with_multiprocessing(texts, num_processes=4):
    # í”„ë¡œì„¸ìŠ¤ë³„ë¡œ ë¶„ë¥˜ê¸° ìƒì„±
    with mp.Pool(num_processes) as pool:
        classifiers = [ToxicClassifier() for _ in range(num_processes)]
        
        # í…ìŠ¤íŠ¸ë¥¼ í”„ë¡œì„¸ìŠ¤ë³„ë¡œ ë¶„í• 
        chunk_size = len(texts) // num_processes
        text_chunks = [texts[i:i+chunk_size] for i in range(0, len(texts), chunk_size)]
        
        # ë³‘ë ¬ ì²˜ë¦¬
        results = []
        for chunk, classifier in zip(text_chunks, classifiers):
            chunk_results = pool.map(partial(predict_worker, classifier=classifier), chunk)
            results.extend(chunk_results)
    
    return results

# ì‚¬ìš© ì˜ˆì‹œ
texts = ["í…ìŠ¤íŠ¸1", "í…ìŠ¤íŠ¸2", ...] * 1000
results = process_with_multiprocessing(texts, num_processes=4)
```

## ê³ ê¸‰ ì‚¬ìš©ë²•

### 1. ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸

```python
class CustomToxicClassifier(ToxicClassifier):
    def build_prompt(self, text):
        return f"ë‹¤ìŒ í•œêµ­ì–´ ëŒ“ê¸€ì´ ì•…ì„± ëŒ“ê¸€ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”:\n\n{text}\n\në‹µë³€:"
    
    def predict(self, text):
        prompt = self.build_prompt(text)
        # ... ë‚˜ë¨¸ì§€ ë¡œì§
```

### 2. ì•™ìƒë¸” ì˜ˆì¸¡

```python
class EnsembleClassifier:
    def __init__(self, model_paths):
        self.classifiers = [ToxicClassifier(path) for path in model_paths]
    
    def predict(self, text):
        predictions = [c.predict(text) for c in self.classifiers]
        
        # íˆ¬í‘œ ê¸°ë°˜ ì˜ˆì¸¡
        toxic_votes = sum(1 for p in predictions if p['prediction'] == 'toxic')
        final_prediction = 'toxic' if toxic_votes > len(predictions) / 2 else 'none'
        
        # í‰ê·  ì‹ ë¢°ë„
        avg_confidence = sum(p['confidence'] for p in predictions) / len(predictions)
        
        return {
            'prediction': final_prediction,
            'confidence': avg_confidence,
            'votes': {
                'toxic': toxic_votes,
                'none': len(predictions) - toxic_votes
            }
        }
```

### 3. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°

```python
import time
from collections import deque

class StreamingClassifier:
    def __init__(self, window_size=100):
        self.classifier = ToxicClassifier()
        self.predictions = deque(maxlen=window_size)
    
    def process_stream(self, text_stream):
        for text in text_stream:
            result = self.classifier.predict(text)
            self.predictions.append(result)
            
            # ì‹¤ì‹œê°„ í†µê³„
            toxic_count = sum(1 for p in self.predictions if p['prediction'] == 'toxic')
            toxic_ratio = toxic_count / len(self.predictions)
            
            yield {
                'text': text,
                'prediction': result['prediction'],
                'confidence': result['confidence'],
                'toxic_ratio': toxic_ratio
            }

# ì‚¬ìš© ì˜ˆì‹œ
def text_stream():
    # ì‹¤ì œë¡œëŠ” ì†Œì¼“ì´ë‚˜ íŒŒì¼ì—ì„œ ì½ì–´ì˜´
    texts = ["í…ìŠ¤íŠ¸1", "í…ìŠ¤íŠ¸2", ...]
    for text in texts:
        yield text
        time.sleep(0.1)

streaming_classifier = StreamingClassifier()
for result in streaming_classifier.process_stream(text_stream()):
    print(f"ì‹¤ì‹œê°„ ê²°ê³¼: {result}")
```

## ë¬¸ì œ í•´ê²°

### 1. ë©”ëª¨ë¦¬ ë¶€ì¡±

```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
classifier = ToxicClassifier(batch_size=1)

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
import torch
torch.cuda.empty_cache()
```

### 2. ëŠë¦° ì¶”ë¡ 

```python
# ì–‘ìí™”ëœ ëª¨ë¸ ì‚¬ìš©
classifier = ToxicClassifier(model_path='quantized-model')

# ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ
results = classifier.predict_batch(texts)
```

### 3. ì •í™•ë„ ê°œì„ 

```python
# ì•™ìƒë¸” ì‚¬ìš©
ensemble = EnsembleClassifier([
    'model-checkpoints/kobert/checkpoint-1100',
    'model-checkpoints/kobert/checkpoint-1000'
])

result = ensemble.predict(text)
```

## ê´€ë ¨ ê°€ì´ë“œ

- **[ë¹ ë¥¸ ì‹œì‘](QUICK_START.md)**: ë¹ ë¥´ê²Œ í”„ë¡œì íŠ¸ ì‹¤í–‰í•˜ê¸°
- **[í•™ìŠµ ê°€ì´ë“œ](TRAINING_GUIDE.md)**: ëª¨ë¸ í•™ìŠµ ê³¼ì • ìƒì„¸ ì„¤ëª…
- **[ì–‘ìí™” ê°€ì´ë“œ](QUANTIZATION_GUIDE.md)**: ëª¨ë¸ ìµœì í™” ë°©ë²•
- **[ê²°ê³¼ ë¶„ì„](results/README.md)**: ì„±ëŠ¥ ë¶„ì„ ë° ë²¤ì¹˜ë§ˆí¬
