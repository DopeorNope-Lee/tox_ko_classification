"""
í•œêµ­ì–´ ì•…ì„± ëŒ“ê¸€ ë¶„ë¥˜ ëª¨ë¸ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í•™ìŠµëœ KoBERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ì•…ì„± ì—¬ë¶€ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.
PEFT(LoRA)ë¡œ íŠœë‹ëœ ëª¨ë¸ì˜ ì¶”ë¡ ì„ ì§€ì›í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python inference.py --text "ë¶„ì„í•  í…ìŠ¤íŠ¸"
    python inference.py --file input.txt
    python inference.py --interactive
"""
import os
# -- í™˜ê²½ ì„¤ì • --
# íŠ¹ì • GPUë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì • (ì˜ˆ: 0ë²ˆ GPU)
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

import argparse
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig
from peft import PeftModel


# -- ëª¨ë¸ ì„¤ì • --
CONFIG = {
    "base_model": "skt/kobert-base-v1",
    "lora_dir": "checkpoints/kobert-lora/checkpoint-700",
}

# -- ë ˆì´ë¸” ì •ì˜ --
# ì˜ˆ: 0: ì•…ì„±, 1: ì •ìƒ
LABEL_MAP = {
    0: "ì•…ì„±",
    1: "ì •ìƒ"
}


def load_model(model_dir: str = CONFIG["lora_dir"]):
    """
    ì‚¬ì „ í•™ìŠµëœ KoBERT ëª¨ë¸ê³¼ LoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë“œí•˜ê³  ë³‘í•©í•©ë‹ˆë‹¤.
    """
    print("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘... ğŸ¢")
    cfg = AutoConfig.from_pretrained(
        CONFIG["base_model"],
        num_labels=len(LABEL_MAP),
        problem_type="single_label_classification"
    )
    tokenizer = AutoTokenizer.from_pretrained(CONFIG["base_model"])
    
    # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
    base_model = AutoModelForSequenceClassification.from_pretrained(
        CONFIG["base_model"],
        config=cfg,
        device_map="auto"
    )
    
    # LoRA ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì™€ ê¸°ë³¸ ëª¨ë¸ê³¼ ë³‘í•©
    model = PeftModel.from_pretrained(base_model, model_dir).merge_and_unload()
    model.eval()
    print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! âœ¨")
    return tokenizer, model


def predict(texts, tokenizer, model):
    """
    ì…ë ¥ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ ì•…ì„± ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    """
    if isinstance(texts, str):
        texts = [texts]

    # í† í¬ë‚˜ì´ì§•
    encodings = tokenizer(
        texts,
        padding=True,
        truncation=True,
        return_tensors="pt",
        max_length=512
    )

    # ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    batch = {
        'input_ids': encodings['input_ids'].to(model.device),
        'attention_mask': encodings['attention_mask'].to(model.device)
    }

    # ì˜ˆì¸¡ ìˆ˜í–‰
    with torch.no_grad():
        outputs = model(**batch)
    
    logits = outputs.logits
    probs = logits.softmax(dim=-1).cpu()
    labels = probs.argmax(dim=-1).tolist()

    # ê²°ê³¼ í¬ë§·íŒ…
    results = []
    for i, text in enumerate(texts):
        label_id = labels[i]
        results.append({
            "text": text,
            "label_id": label_id,
            "label_name": LABEL_MAP.get(label_id, "ì•Œ ìˆ˜ ì—†ìŒ"),
            "probability": float(probs[i, label_id])
        })
        
    return results


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜: ì»¤ë§¨ë“œ ë¼ì¸ ì¸ìë¥¼ íŒŒì‹±í•˜ê³  ì ì ˆí•œ ëª¨ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    parser = argparse.ArgumentParser(description="í•œêµ­ì–´ ì•…ì„± ëŒ“ê¸€ ë¶„ë¥˜ ëª¨ë¸ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸")
    
    # ì„¸ ê°€ì§€ ëª¨ë“œëŠ” ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ë„ë¡ ì„¤ì •
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--text", type=str, help="ë¶„ë¥˜í•  ë‹¨ì¼ í…ìŠ¤íŠ¸")
    group.add_argument("--file", type=str, help="ë¶„ë¥˜í•  í…ìŠ¤íŠ¸ê°€ ë‹´ê¸´ íŒŒì¼ ê²½ë¡œ (í•œ ì¤„ì— í•œ í…ìŠ¤íŠ¸)")
    group.add_argument("--interactive", action="store_true", help="ëŒ€í™”í˜• ëª¨ë“œë¡œ ì‹¤í–‰")

    args = parser.parse_args()

    # ëª¨ë¸ ë¡œë“œ (í•œ ë²ˆë§Œ ì‹¤í–‰)
    tokenizer, model = load_model()

    if args.text:
        # -- ë‹¨ì¼ í…ìŠ¤íŠ¸ ëª¨ë“œ --
        results = predict(args.text, tokenizer, model)
        for res in results:
            print(f"ğŸ’¬ ì…ë ¥: \"{res['text']}\"")
            print(f"âœ… ê²°ê³¼: {res['label_name']} (í™•ë¥ : {res['probability']:.2%})")

    elif args.file:
        # -- íŒŒì¼ ëª¨ë“œ --
        try:
            with open(args.file, 'r', encoding='utf-8') as f:
                lines = [line.strip() for line in f if line.strip()]
            
            if not lines:
                print("íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            print(f"ì´ {len(lines)}ê°œì˜ í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ì—ì„œ ì½ì—ˆìŠµë‹ˆë‹¤. ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            results = predict(lines, tokenizer, model)
            for res in results:
                print("-" * 30)
                print(f"ğŸ’¬ ì…ë ¥: \"{res['text']}\"")
                print(f"âœ… ê²°ê³¼: {res['label_name']} (í™•ë¥ : {res['probability']:.2%})")

        except FileNotFoundError:
            print(f"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. -> {args.file}")
        except Exception as e:
            print(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    elif args.interactive:
        # -- ëŒ€í™”í˜• ëª¨ë“œ --
        print("\nëŒ€í™”í˜• ëª¨ë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ë¶„ì„í•  ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”. (ì¢…ë£Œ: 'exit' ë˜ëŠ” 'quit')")
        while True:
            try:
                user_input = input(">>> ")
                if user_input.lower() in ["exit", "quit"]:
                    print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                if not user_input.strip():
                    continue
                
                results = predict(user_input, tokenizer, model)
                for res in results:
                    print(f"âœ… ê²°ê³¼: {res['label_name']} (í™•ë¥ : {res['probability']:.2%})\n")

            except (KeyboardInterrupt, EOFError):
                print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break


if __name__ == "__main__":
    main()
