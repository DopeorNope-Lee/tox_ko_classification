#!/usr/bin/env python3
"""
í•œêµ­ì–´ ì•…ì„± ëŒ“ê¸€ ë¶„ë¥˜ ëª¨ë¸ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í•™ìŠµëœ KoBERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ì•…ì„± ì—¬ë¶€ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.
ì–‘ìí™”ëœ ëª¨ë¸ê³¼ ì¼ë°˜ ëª¨ë¸ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
Windows 11 í™˜ê²½ì˜ GPU ë‚´ì¥ ë…¸íŠ¸ë¶ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ì‚¬ìš©í•˜ë ¤ë©´ ì´ íŒŒì¼ í•˜ë‹¨ì˜ ì„¤ì •ê°’ë“¤ì„ ìˆ˜ì •í•œ í›„ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.

ì‚¬ìš© ì „ì— setup.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ í™˜ê²½ì„ ì„¤ì •í•˜ì„¸ìš”.

ì£¼ì˜ì‚¬í•­:
    - ë…¸íŠ¸ë¶ GPU ë©”ëª¨ë¦¬ ì œì•½ì„ ê³ ë ¤í•œ ì„¤ì •ì´ ì ìš©ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    - ì–‘ìí™”ëœ ëª¨ë¸ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤ (ë©”ëª¨ë¦¬ ì ˆì•½).
"""

import torch
import json
from pathlib import Path
from typing import Dict, List, Union
from transformers import AutoModelForSequenceClassification, AutoTokenizer


class ToxicCommentClassifier:
    """
    í•œêµ­ì–´ ì•…ì„± ëŒ“ê¸€ ë¶„ë¥˜ê¸° (Windows ë…¸íŠ¸ë¶ ìµœì í™”)
    
    í•™ìŠµëœ KoBERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ì•…ì„± ì—¬ë¶€ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, model_path: str = "bnb-4bit", device: str = "auto"):
        """
        ë¶„ë¥˜ê¸° ì´ˆê¸°í™”
        
        Args:
            model_path (str): ëª¨ë¸ ê²½ë¡œ (ê¸°ë³¸ê°’: "bnb-4bit")
            device (str): ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (ê¸°ë³¸ê°’: "auto")
        """
        self.model_path = model_path
        self.device = device
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        print(f"ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")
        
        # Windows ë…¸íŠ¸ë¶ ìµœì í™” ì„¤ì •
        model_kwargs = {
            "device_map": device,
            "torch_dtype": torch.float16,  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ 16ë¹„íŠ¸ ì‚¬ìš©
        }
        
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_path, 
            **model_kwargs
        )
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # ë¼ë²¨ ë§¤í•‘ ì„¤ì •
        self.id2label = {0: "toxic", 1: "none"}
        self.label2id = {"toxic": 0, "none": 1}
        
        print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB")
    
    def build_prompt(self, text: str) -> str:
        """
        ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            text (str): ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            str: í”„ë¡¬í”„íŠ¸ í˜•ì‹ì˜ í…ìŠ¤íŠ¸
        """
        return (
            "ë‹¤ìŒ ë¬¸ì¥ì´ ê¸ì •ì¸ì§€ ë¶€ì •ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.\n\n"
            "### ë¬¸ì¥:\n"
            f"{text}"
        )
    
    def predict(self, text: str, return_confidence: bool = True) -> Dict[str, Union[str, float]]:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰
        
        Args:
            text (str): ì˜ˆì¸¡í•  í…ìŠ¤íŠ¸
            return_confidence (bool): ì‹ ë¢°ë„ ë°˜í™˜ ì—¬ë¶€
            
        Returns:
            Dict: ì˜ˆì¸¡ ê²°ê³¼ (prediction, confidence)
        """
        # í”„ë¡¬í”„íŠ¸ ì ìš©
        prompt_text = self.build_prompt(text)
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompt_text, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512
        )
        
        # GPUë¡œ ì´ë™ (í•„ìš”ì‹œ)
        if self.device != "auto":
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # ì˜ˆì¸¡
        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = torch.softmax(outputs.logits, dim=-1)
            predicted_class = torch.argmax(predictions, dim=-1).item()
            confidence = predictions[0][predicted_class].item()
        
        result = {
            "prediction": self.id2label[predicted_class],
            "confidence": confidence
        }
        
        return result
    
    def predict_batch(self, texts: List[str], return_confidence: bool = True) -> List[Dict[str, Union[str, float]]]:
        """
        ë°°ì¹˜ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰ (ë…¸íŠ¸ë¶ ë©”ëª¨ë¦¬ ìµœì í™”)
        
        Args:
            texts (List[str]): ì˜ˆì¸¡í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            return_confidence (bool): ì‹ ë¢°ë„ ë°˜í™˜ ì—¬ë¶€
            
        Returns:
            List[Dict]: ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        # ë…¸íŠ¸ë¶ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì‘ì€ ë°°ì¹˜ë¡œ ì²˜ë¦¬
        batch_size = 4  # ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            for text in batch_texts:
                result = self.predict(text, return_confidence)
                results.append(result)
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        return results


def load_texts_from_file(file_path: str) -> List[str]:
    """
    íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ (Windows ê²½ë¡œ ì§€ì›)
    
    Args:
        file_path (str): í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        List[str]: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    # Windows ê²½ë¡œ ì •ê·œí™”
    file_path = Path(file_path)
    
    with open(file_path, 'r', encoding='utf-8') as f:
        texts = [line.strip() for line in f if line.strip()]
    return texts


def save_results(results: List[Dict], output_path: str):
    """
    ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (Windows ê²½ë¡œ ì§€ì›)
    
    Args:
        results (List[Dict]): ì €ì¥í•  ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        output_path (str): ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    """
    # Windows ê²½ë¡œ ì •ê·œí™”
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ê²°ê³¼ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def interactive_mode(classifier: ToxicCommentClassifier):
    """
    ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰ (Windows ìµœì í™”)
    
    Args:
        classifier: ì´ˆê¸°í™”ëœ ë¶„ë¥˜ê¸°
    """
    print("\n=== ëŒ€í™”í˜• ëª¨ë“œ (Windows ìµœì í™”) ===")
    print("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ ì•…ì„± ì—¬ë¶€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. (ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit')")
    print("ğŸ’¡ ë…¸íŠ¸ë¶ ìµœì í™”: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤.")
    
    while True:
        try:
            text = input("\ní…ìŠ¤íŠ¸ ì…ë ¥: ").strip()
            
            if text.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                print("ëŒ€í™”í˜• ëª¨ë“œë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not text:
                print("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            result = classifier.predict(text)
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"ì˜ˆì¸¡ ê²°ê³¼: {result['prediction']}")
            print(f"ì‹ ë¢°ë„: {result['confidence']:.3f}")
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í‘œì‹œ
            if torch.cuda.is_available():
                gpu_memory_used = torch.cuda.memory_allocated() / 1024**2
                print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {gpu_memory_used:.1f}MB")
            
        except KeyboardInterrupt:
            print("\nëŒ€í™”í˜• ëª¨ë“œë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")


def single_text_prediction(classifier: ToxicCommentClassifier, text: str):
    """
    ë‹¨ì¼ í…ìŠ¤íŠ¸ ì˜ˆì¸¡
    
    Args:
        classifier: ì´ˆê¸°í™”ëœ ë¶„ë¥˜ê¸°
        text (str): ì˜ˆì¸¡í•  í…ìŠ¤íŠ¸
    """
    result = classifier.predict(text)
    print(f"\nì…ë ¥ í…ìŠ¤íŠ¸: {text}")
    print(f"ì˜ˆì¸¡ ê²°ê³¼: {result['prediction']}")
    print(f"ì‹ ë¢°ë„: {result['confidence']:.3f}")
    
    # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í‘œì‹œ
    if torch.cuda.is_available():
        gpu_memory_used = torch.cuda.memory_allocated() / 1024**2
        print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {gpu_memory_used:.1f}MB")
    
    return result


def batch_file_prediction(classifier: ToxicCommentClassifier, file_path: str, output_path: str = None):
    """
    íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì½ì–´ ë°°ì¹˜ ì˜ˆì¸¡ ìˆ˜í–‰ (Windows ìµœì í™”)
    
    Args:
        classifier: ì´ˆê¸°í™”ëœ ë¶„ë¥˜ê¸°
        file_path (str): í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        output_path (str): ê²°ê³¼ ì €ì¥ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
    """
    try:
        texts = load_texts_from_file(file_path)
        print(f"íŒŒì¼ì—ì„œ {len(texts)}ê°œì˜ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        
        results = classifier.predict_batch(texts)
        
        # ê²°ê³¼ ì¶œë ¥
        for i, (text, result) in enumerate(zip(texts, results), 1):
            print(f"\n{i}. í…ìŠ¤íŠ¸: {text}")
            print(f"   ì˜ˆì¸¡: {result['prediction']} (ì‹ ë¢°ë„: {result['confidence']:.3f})")
        
        if output_path:
            save_results(results, output_path)
            
        return results
            
    except FileNotFoundError:
        print(f"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return None
    except Exception as e:
        print(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None


# ============================================================================
# ì¶”ë¡  ì„¤ì •ê°’ - ì—¬ê¸°ì„œ ì§ì ‘ ìˆ˜ì •í•˜ì„¸ìš” (Windows ë…¸íŠ¸ë¶ ìµœì í™”)
# ============================================================================

# ëª¨ë¸ ì„¤ì •
MODEL_PATH = "bnb-4bit"  # ëª¨ë¸ ê²½ë¡œ (ì–‘ìí™”ëœ ëª¨ë¸ ê¶Œì¥)
DEVICE = "auto"  # ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤

# ì‹¤í–‰ ëª¨ë“œ ì„¤ì • (í•˜ë‚˜ë§Œ Trueë¡œ ì„¤ì •)
SINGLE_TEXT_MODE = False  # ë‹¨ì¼ í…ìŠ¤íŠ¸ ì˜ˆì¸¡
BATCH_FILE_MODE = False   # íŒŒì¼ì—ì„œ ë°°ì¹˜ ì˜ˆì¸¡
INTERACTIVE_MODE = True   # ëŒ€í™”í˜• ëª¨ë“œ

# ë‹¨ì¼ í…ìŠ¤íŠ¸ ëª¨ë“œ ì„¤ì •
INPUT_TEXT = "ì•ˆë…•í•˜ì„¸ìš”! ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”."

# ë°°ì¹˜ íŒŒì¼ ëª¨ë“œ ì„¤ì • (Windows ê²½ë¡œ)
INPUT_FILE = "input.txt"  # ì…ë ¥ íŒŒì¼ ê²½ë¡œ
OUTPUT_FILE = "results.json"  # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ

# ============================================================================
# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
# ============================================================================

if __name__ == "__main__":
    print("ğŸš€ í•œêµ­ì–´ ì•…ì„± ëŒ“ê¸€ ë¶„ë¥˜ ëª¨ë¸ ì¶”ë¡  (Windows 11 + GPU ë…¸íŠ¸ë¶)")
    print("=" * 60)
    print()
    
    # ëª¨ë¸ ê²½ë¡œ í™•ì¸ (Windows ê²½ë¡œ ì§€ì›)
    model_path = Path(MODEL_PATH)
    if not model_path.exists():
        print(f"ì˜¤ë¥˜: ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MODEL_PATH}")
        print("ë¨¼ì € train.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ê±°ë‚˜ quantization.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ì–‘ìí™”ëœ ëª¨ë¸ì„ ìƒì„±í•˜ì„¸ìš”.")
        print("ğŸ’¡ ë…¸íŠ¸ë¶ì—ì„œëŠ” ì–‘ìí™”ëœ ëª¨ë¸(bnb-4bit) ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        exit(1)
    
    # ë¶„ë¥˜ê¸° ì´ˆê¸°í™”
    try:
        classifier = ToxicCommentClassifier(MODEL_PATH, DEVICE)
    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œ ê²½ìš° ì–‘ìí™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        exit(1)
    
    # ì‹¤í–‰ ëª¨ë“œ ê²°ì •
    if INTERACTIVE_MODE:
        interactive_mode(classifier)
    elif SINGLE_TEXT_MODE:
        single_text_prediction(classifier, INPUT_TEXT)
    elif BATCH_FILE_MODE:
        batch_file_prediction(classifier, INPUT_FILE, OUTPUT_FILE)
    else:
        print("ì‹¤í–‰ ëª¨ë“œë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”. (INTERACTIVE_MODE, SINGLE_TEXT_MODE, BATCH_FILE_MODE ì¤‘ í•˜ë‚˜ë¥¼ Trueë¡œ ì„¤ì •)")
    
    print("\nğŸ’¡ Windows ë…¸íŠ¸ë¶ ìµœì í™” íŒ:")
    print("   - GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ë©´ ì–‘ìí™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”")
    print("   - ì¶”ë¡  ì¤‘ì—ëŠ” ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•˜ì—¬ GPU ë©”ëª¨ë¦¬ë¥¼ í™•ë³´í•˜ì„¸ìš”")
    print("   - ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ì‹œ ë°°ì¹˜ í¬ê¸°ê°€ ìë™ìœ¼ë¡œ ì¡°ì •ë©ë‹ˆë‹¤") 