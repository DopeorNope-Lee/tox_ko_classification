#!/usr/bin/env python3
"""
λ¨λΈ μ–‘μν™” μ¤ν¬λ¦½νΈ (Windows 11 + GPU λ…ΈνΈλ¶)

μ΄ μ¤ν¬λ¦½νΈλ” ν•™μµλ ν•κµ­μ–΄ μ•…μ„± λ“κΈ€ λ¶„λ¥ λ¨λΈμ„ 4-bit μ–‘μν™”ν•μ—¬ 
λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ„ μ¤„μ΄κ³  μ¶”λ΅  μ†λ„λ¥Ό ν–¥μƒμ‹ν‚µλ‹λ‹¤.
Windows 11 ν™κ²½μ GPU λ‚΄μ¥ λ…ΈνΈλ¶μ— μµμ ν™”λμ–΄ μμµλ‹λ‹¤.

μ–‘μν™”λ¥Ό μ‹μ‘ν•λ ¤λ©΄ μ΄ νμΌ ν•λ‹¨μ μ„¤μ •κ°’λ“¤μ„ μμ •ν• ν›„ μ¤ν¬λ¦½νΈλ¥Ό μ‹¤ν–‰ν•μ„Έμ”.

μ‚¬μ© μ „μ— setup.pyλ¥Ό λ¨Όμ € μ‹¤ν–‰ν•μ—¬ ν™κ²½μ„ μ„¤μ •ν•μ„Έμ”.

μ£Όμμ‚¬ν•­:
    - λ…ΈνΈλ¶ GPU λ©”λ¨λ¦¬ μ μ•½μ„ κ³ λ ¤ν• μ„¤μ •μ΄ μ μ©λμ–΄ μμµλ‹λ‹¤.
    - μ–‘μν™”λ” λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ„ ν¬κ² μ¤„μ—¬ λ…ΈνΈλ¶μ—μ„ μ¶”λ΅ μ„ μ©μ΄ν•κ² ν•©λ‹λ‹¤.
    - μ–‘μν™” κ³Όμ •μ—μ„ GPU λ©”λ¨λ¦¬κ°€ λ§μ΄ μ‚¬μ©λ  μ μμµλ‹λ‹¤.
"""

import torch
import shutil
import tempfile
from pathlib import Path
from typing import Dict, List, Union

# Transformers λΌμ΄λΈλ¬λ¦¬ import
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    AutoConfig,
)
from peft import PeftModel, PeftConfig


def load_base_model(base_model_name: str = "skt/kobert-base-v1"):
    """
    κΈ°λ³Έ KoBERT λ¨λΈ λ΅λ“ (λ…ΈνΈλ¶ μµμ ν™”)
    
    Args:
        base_model_name (str): κΈ°λ³Έ λ¨λΈ κ²½λ΅
        
    Returns:
        tuple: (base_model, config)
    """
    print("=== κΈ°λ³Έ λ¨λΈ λ΅λ“ (λ…ΈνΈλ¶ μµμ ν™”) ===")
    
    # λ¨λΈ μ„¤μ • (2κ° ν΄λμ¤ λ¶„λ¥)
    config = AutoConfig.from_pretrained(
        base_model_name, 
        num_labels=2, 
        problem_type="single_label_classification"
    )
    
    # λ…ΈνΈλ¶ GPU λ©”λ¨λ¦¬ μ μ•½μ„ μ„ν• μ„¤μ •
    model_kwargs = {
        "torch_dtype": torch.float16,  # 16λΉ„νΈ μ •λ°€λ„λ΅ λ©”λ¨λ¦¬ μ μ•½
        "low_cpu_mem_usage": True,     # CPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μµμ†ν™”
    }
    
    # κΈ°λ³Έ λ¨λΈ λ΅λ“
    base_model = AutoModelForSequenceClassification.from_pretrained(
        base_model_name, 
        config=config,
        **model_kwargs
    )
    
    print(f"κΈ°λ³Έ λ¨λΈ λ΅λ“ μ™„λ£: {base_model_name}")
    print(f"λ¨λΈ νƒ€μ…: {type(base_model)}")
    print(f"νλΌλ―Έν„° μ: {sum(p.numel() for p in base_model.parameters()):,}")
    
    # GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰ ν™•μΈ
    if torch.cuda.is_available():
        gpu_memory_used = torch.cuda.memory_allocated() / 1024**2
        print(f"GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰: {gpu_memory_used:.1f}MB")
    
    return base_model, config


def load_and_merge_lora_model(base_model, checkpoint_path: str):
    """
    ν•™μµλ LoRA λ¨λΈ λ΅λ“ λ° λ³‘ν•© (λ…ΈνΈλ¶ μµμ ν™”)
    
    Args:
        base_model: κΈ°λ³Έ λ¨λΈ
        checkpoint_path (str): LoRA μ²΄ν¬ν¬μΈνΈ κ²½λ΅
        
    Returns:
        λ³‘ν•©λ λ¨λΈ
    """
    print(f"=== LoRA λ¨λΈ λ΅λ“ λ° λ³‘ν•© (λ…ΈνΈλ¶ μµμ ν™”): {checkpoint_path} ===")
    
    # μ²΄ν¬ν¬μΈνΈ κ²½λ΅ ν™•μΈ (Windows κ²½λ΅ μ§€μ›)
    checkpoint_path = Path(checkpoint_path)
    if not checkpoint_path.exists():
        raise FileNotFoundError(f"μ²΄ν¬ν¬μΈνΈλ¥Ό μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {checkpoint_path}")
    
    # LoRA λ¨λΈ λ΅λ“ λ° λ³‘ν•©
    model = PeftModel.from_pretrained(base_model, checkpoint_path).merge_and_unload()
    
    print("LoRA λ¨λΈ λ΅λ“ λ° λ³‘ν•© μ™„λ£")
    print(f"λ³‘ν•©λ λ¨λΈ νƒ€μ…: {type(model)}")
    print(f"νλΌλ―Έν„° μ: {sum(p.numel() for p in model.parameters()):,}")
    
    # GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰ ν™•μΈ
    if torch.cuda.is_available():
        gpu_memory_used = torch.cuda.memory_allocated() / 1024**2
        print(f"GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰: {gpu_memory_used:.1f}MB")
    
    return model


def apply_quantization(model, save_dir: str = "bnb-4bit"):
    """
    4-bit μ–‘μν™” μ μ© (λ…ΈνΈλ¶ μµμ ν™”)
    
    Args:
        model: λ³‘ν•©λ λ¨λΈ
        save_dir (str): μ–‘μν™”λ λ¨λΈ μ €μ¥ κ²½λ΅
        
    Returns:
        μ–‘μν™”λ λ¨λΈ
    """
    print("=== 4-bit μ–‘μν™” μ μ© (λ…ΈνΈλ¶ μµμ ν™”) ===")
    
    # μ„μ‹ λ””λ ‰ν† λ¦¬λ¥Ό μ‚¬μ©ν•μ—¬ μ–‘μν™” κ³Όμ • μν–‰
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        
        # λ¨Όμ € fp16/fp32 ν•μ‹μΌλ΅ μ €μ¥
        model.save_pretrained(tmp_path)
        print("μ„μ‹ λ¨λΈ μ €μ¥ μ™„λ£")
        
        # GPU λ©”λ¨λ¦¬ μ •λ¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("GPU λ©”λ¨λ¦¬ μ •λ¦¬ μ™„λ£")
        
        # 4-bit μ–‘μν™” μ μ©ν•μ—¬ λ‹¤μ‹ λ΅λ“
        quantization_config = {
            "load_in_4bit": True,
            "device_map": "auto",
            "torch_dtype": torch.float16,
            "low_cpu_mem_usage": True,
        }
        
        model = AutoModelForSequenceClassification.from_pretrained(
            tmp_path, 
            **quantization_config
        )
        
        print("4-bit μ–‘μν™” μ μ© μ™„λ£")
        print(f"μ–‘μν™”λ λ¨λΈ νƒ€μ…: {type(model)}")
        
        # GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰ ν™•μΈ
        if torch.cuda.is_available():
            gpu_memory_used = torch.cuda.memory_allocated() / 1024**2
            print(f"μ–‘μν™” ν›„ GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰: {gpu_memory_used:.1f}MB")
    
    return model


def save_quantized_model(model, tokenizer, save_dir: str = "bnb-4bit"):
    """
    μ–‘μν™”λ λ¨λΈ μ €μ¥ (Windows κ²½λ΅ μ§€μ›)
    
    Args:
        model: μ–‘μν™”λ λ¨λΈ
        tokenizer: ν† ν¬λ‚μ΄μ €
        save_dir (str): μ €μ¥ κ²½λ΅
    """
    print(f"=== μ–‘μν™”λ λ¨λΈ μ €μ¥ (Windows κ²½λ΅): {save_dir} ===")
    
    # Windows κ²½λ΅ μ •κ·ν™”
    save_dir = Path(save_dir)
    
    # μ €μ¥ λ””λ ‰ν† λ¦¬ μƒμ„±
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # ν† ν¬λ‚μ΄μ € μ €μ¥
    tokenizer.save_pretrained(save_dir)
    
    # μ–‘μν™”λ λ¨λΈ μ €μ¥
    model.save_pretrained(save_dir)
    
    print(f"μ–‘μν™”λ λ¨λΈ μ €μ¥ μ™„λ£: {save_dir}")
    
    # μ €μ¥λ νμΌλ“¤ ν™•μΈ
    print("\nμ €μ¥λ νμΌλ“¤:")
    for file in save_dir.glob("*"):
        print(f"- {file.name}")
    
    # μ €μ¥λ λ¨λΈ ν¬κΈ° ν™•μΈ
    model_size_mb = get_model_size_mb(str(save_dir))
    print(f"\nμ €μ¥λ λ¨λΈ ν¬κΈ°: {model_size_mb:.1f}MB")


def get_model_size_mb(model_path: str) -> float:
    """
    λ¨λΈ νμΌμ ν¬κΈ°λ¥Ό MB λ‹¨μ„λ΅ κ³„μ‚°
    
    Args:
        model_path (str): λ¨λΈ κ²½λ΅
        
    Returns:
        float: λ¨λΈ ν¬κΈ° (MB)
    """
    total_size = 0
    model_path = Path(model_path)
    for file_path in model_path.rglob("*.bin"):
        total_size += file_path.stat().st_size
    return total_size / (1024 * 1024)  # MBλ΅ λ³€ν™


def compare_model_sizes(original_size_mb: float, quantized_path: str):
    """
    λ¨λΈ ν¬κΈ° λΉ„κµ
    
    Args:
        original_size_mb (float): μ›λ³Έ λ¨λΈ ν¬κΈ° (MB)
        quantized_path (str): μ–‘μν™”λ λ¨λΈ κ²½λ΅
    """
    print("=== λ¨λΈ ν¬κΈ° λΉ„κµ ===")
    
    # μ–‘μν™”λ λ¨λΈ ν¬κΈ°
    quantized_size_mb = get_model_size_mb(quantized_path)
    
    print(f"μ›λ³Έ λ¨λΈ (μμƒ): {original_size_mb:.1f} MB")
    print(f"μ–‘μν™”λ λ¨λΈ: {quantized_size_mb:.1f} MB")
    print(f"ν¬κΈ° κ°μ†μ¨: {((original_size_mb - quantized_size_mb) / original_size_mb * 100):.1f}%")
    
    # λ…ΈνΈλ¶ μµμ ν™” μ •λ³΄
    print(f"\nπ’΅ λ…ΈνΈλ¶ μµμ ν™” ν¨κ³Ό:")
    print(f"   - λ©”λ¨λ¦¬ μ‚¬μ©λ‰: μ•½ {((original_size_mb - quantized_size_mb) / original_size_mb * 100):.0f}% μ μ•½")
    print(f"   - μ¶”λ΅  μ†λ„: λ” λΉ λ¥Έ λ΅λ”© λ° μ¶”λ΅ ")
    print(f"   - λ°°ν„°λ¦¬ ν¨μ¨: λ” λ‚®μ€ μ „λ ¥ μ†λ¨")


def test_quantized_model(model_path: str, test_texts: List[str]):
    """
    μ–‘μν™”λ λ¨λΈ ν…μ¤νΈ (λ…ΈνΈλ¶ μµμ ν™”)
    
    Args:
        model_path (str): λ¨λΈ κ²½λ΅
        test_texts (List[str]): ν…μ¤νΈν•  ν…μ¤νΈ λ¦¬μ¤νΈ
        
    Returns:
        List[Dict]: μμΈ΅ κ²°κ³Ό λ¦¬μ¤νΈ
    """
    print("=== μ–‘μν™”λ λ¨λΈ ν…μ¤νΈ (λ…ΈνΈλ¶ μµμ ν™”) ===")
    
    # λ¨λΈκ³Ό ν† ν¬λ‚μ΄μ € λ΅λ“
    model = AutoModelForSequenceClassification.from_pretrained(
        model_path, 
        device_map="auto",
        torch_dtype=torch.float16
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    results = []
    
    for i, text in enumerate(test_texts, 1):
        print(f"\nν…μ¤νΈ {i}/{len(test_texts)}:")
        
        # ν”„λ΅¬ν”„νΈ μ μ©
        prompt_text = f"λ‹¤μ λ¬Έμ¥μ΄ κΈμ •μΈμ§€ λ¶€μ •μΈμ§€ νλ‹¨ν•μ„Έμ”.\n\n### λ¬Έμ¥:\n{text}"
        
        # ν† ν¬λ‚μ΄μ§•
        inputs = tokenizer(prompt_text, return_tensors="pt", truncation=True, max_length=512)
        
        # μμΈ΅
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.softmax(outputs.logits, dim=-1)
            predicted_class = torch.argmax(predictions, dim=-1).item()
        
        label = "toxic" if predicted_class == 0 else "none"
        confidence = predictions[0][predicted_class].item()
        
        results.append({
            'text': text,
            'prediction': label,
            'confidence': confidence
        })
        
        print(f"ν…μ¤νΈ: {text}")
        print(f"μμΈ΅: {label} (μ‹ λΆ°λ„: {confidence:.3f})")
        
        # GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰ ν‘μ‹
        if torch.cuda.is_available():
            gpu_memory_used = torch.cuda.memory_allocated() / 1024**2
            print(f"GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰: {gpu_memory_used:.1f}MB")
    
    return results


def generate_usage_example(save_dir: str = "bnb-4bit"):
    """
    μ‚¬μ©λ²• μμ‹ μ½”λ“ μƒμ„± (Windows μµμ ν™”)
    
    Args:
        save_dir (str): λ¨λΈ μ €μ¥ κ²½λ΅
    """
    usage_example = f'''
# μ–‘μν™”λ λ¨λΈ μ‚¬μ© μμ‹ (Windows λ…ΈνΈλ¶ μµμ ν™”)
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# λ¨λΈκ³Ό ν† ν¬λ‚μ΄μ € λ΅λ“ (λ…ΈνΈλ¶ μµμ ν™”)
model = AutoModelForSequenceClassification.from_pretrained(
    '{save_dir}', 
    device_map='auto',
    torch_dtype=torch.float16  # λ©”λ¨λ¦¬ μ μ•½
)
tokenizer = AutoTokenizer.from_pretrained('{save_dir}')

# μμΈ΅ ν•¨μ
def predict_toxic(text):
    prompt = f"λ‹¤μ λ¬Έμ¥μ΄ κΈμ •μΈμ§€ λ¶€μ •μΈμ§€ νλ‹¨ν•μ„Έμ”.\\n\\n### λ¬Έμ¥:\\n{{text}}"
    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=512)
    
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.softmax(outputs.logits, dim=-1)
        predicted_class = torch.argmax(predictions, dim=-1).item()
    
    return "toxic" if predicted_class == 0 else "none"

# μ‚¬μ© μμ‹
result = predict_toxic("ν…μ¤νΈ λ¬Έμ¥")
print(f"κ²°κ³Ό: {{result}}")

# GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰ ν™•μΈ
if torch.cuda.is_available():
    gpu_memory = torch.cuda.memory_allocated() / 1024**2
    print(f"GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰: {{gpu_memory:.1f}}MB")
'''
    
    print("=== μ–‘μν™”λ λ¨λΈ μ‚¬μ©λ²• (Windows λ…ΈνΈλ¶ μµμ ν™”) ===")
    print(usage_example)
    
    # μ‚¬μ©λ²•μ„ νμΌλ΅ μ €μ¥ (Windows κ²½λ΅)
    usage_file = Path(save_dir) / "usage_example.py"
    with open(usage_file, 'w', encoding='utf-8') as f:
        f.write(usage_example)
    
    print(f"μ‚¬μ©λ²• μμ‹κ°€ {usage_file}μ— μ €μ¥λμ—μµλ‹λ‹¤.")


# ============================================================================
# μ–‘μν™” μ„¤μ •κ°’ - μ—¬κΈ°μ„ μ§μ ‘ μμ •ν•μ„Έμ” (Windows λ…ΈνΈλ¶ μµμ ν™”)
# ============================================================================

# ν•„μ μ„¤μ •
CHECKPOINT_PATH = "model-checkpoints\\kobert\\checkpoint-1100"  # LoRA μ²΄ν¬ν¬μΈνΈ κ²½λ΅ (Windows κ²½λ΅)

# μ„ νƒμ  μ„¤μ •
BASE_MODEL = "skt/kobert-base-v1"  # κΈ°λ³Έ λ¨λΈ κ²½λ΅
SAVE_DIR = "bnb-4bit"  # μ–‘μν™”λ λ¨λΈ μ €μ¥ κ²½λ΅

# ν…μ¤νΈ μ„¤μ •
RUN_TEST = True  # μ–‘μν™”λ λ¨λΈ ν…μ¤νΈ μ‹¤ν–‰ μ—¬λ¶€
TEST_TEXTS = [
    "μ•λ…•ν•μ„Έμ”! μΆ‹μ€ ν•λ£¨ λμ„Έμ”.",
    "μ΄λ° λ§μ€ ν•λ©΄ μ• λ©λ‹λ‹¤.",
    "μ •λ§ λ©‹μ§„ ν”„λ΅μ νΈλ„¤μ”!"
]

# ============================================================================
# λ©”μΈ μ‹¤ν–‰ μ½”λ“
# ============================================================================

if __name__ == "__main__":
    print("π€ ν•κµ­μ–΄ μ•…μ„± λ“κΈ€ λ¶„λ¥ λ¨λΈ μ–‘μν™” (Windows 11 + GPU λ…ΈνΈλ¶)")
    print("=" * 70)
    print()
    
    try:
        # κΈ°λ³Έ λ¨λΈ λ΅λ“
        base_model, config = load_base_model(BASE_MODEL)
        
        # LoRA λ¨λΈ λ΅λ“ λ° λ³‘ν•©
        model = load_and_merge_lora_model(base_model, CHECKPOINT_PATH)
        
        # μ–‘μν™” μ μ©
        quantized_model = apply_quantization(model, SAVE_DIR)
        
        # ν† ν¬λ‚μ΄μ € λ΅λ“
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)
        
        # μ–‘μν™”λ λ¨λΈ μ €μ¥
        save_quantized_model(quantized_model, tokenizer, SAVE_DIR)
        
        # λ¨λΈ ν¬κΈ° λΉ„κµ
        original_size_mb = 420  # KoBERT κΈ°λ³Έ λ¨λΈ ν¬κΈ°
        compare_model_sizes(original_size_mb, SAVE_DIR)
        
        # ν…μ¤νΈ μ‹¤ν–‰ (μ„ νƒμ‚¬ν•­)
        if RUN_TEST:
            test_quantized_model(SAVE_DIR, TEST_TEXTS)
        
        # μ‚¬μ©λ²• μμ‹ μƒμ„±
        generate_usage_example(SAVE_DIR)
        
        print("\n" + "=" * 70)
        print("=== μ–‘μν™” μ™„λ£ ===")
        print(f"μ–‘μν™”λ λ¨λΈμ΄ {SAVE_DIR}μ— μ €μ¥λμ—μµλ‹λ‹¤.")
        print("μ΄μ  inference.py μ¤ν¬λ¦½νΈλ‚ μ‚¬μ©λ²• μμ‹λ¥Ό μ°Έκ³ ν•μ—¬ λ¨λΈμ„ μ‚¬μ©ν•  μ μμµλ‹λ‹¤.")
        print()
        print("π’΅ Windows λ…ΈνΈλ¶ μµμ ν™” ν¨κ³Ό:")
        print("   - λ©”λ¨λ¦¬ μ‚¬μ©λ‰ λ€ν­ κ°μ†")
        print("   - μ¶”λ΅  μ†λ„ ν–¥μƒ")
        print("   - λ°°ν„°λ¦¬ ν¨μ¨μ„± κ°μ„ ")
        print("   - λ” μ•μ •μ μΈ μ‹¤ν–‰")
        
    except Exception as e:
        print(f"μ¤λ¥ λ°μƒ: {e}")
        print("μ²΄ν¬ν¬μΈνΈ κ²½λ΅λ¥Ό ν™•μΈν•κ³  λ‹¤μ‹ μ‹λ„ν•΄μ£Όμ„Έμ”.")
        print("π’΅ λ…ΈνΈλ¶ GPU λ©”λ¨λ¦¬κ°€ λ¶€μ΅±ν• κ²½μ° λ‹¤λ¥Έ ν”„λ΅κ·Έλ¨μ„ μΆ…λ£ν•κ³  λ‹¤μ‹ μ‹λ„ν•μ„Έμ”.")
        exit(1) 