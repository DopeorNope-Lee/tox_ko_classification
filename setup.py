#!/usr/bin/env python3
"""
ν•κµ­μ–΄ μ•…μ„± λ“κΈ€ λ¶„λ¥ ν”„λ΅μ νΈ ν™κ²½ μ„¤μ • μ¤ν¬λ¦½νΈ

μ΄ μ¤ν¬λ¦½νΈλ” Windows 11 ν™κ²½μ GPU λ‚΄μ¥ λ…ΈνΈλ¶μ—μ„ ν”„λ΅μ νΈ μ‹¤ν–‰μ— ν•„μ”ν• ν™κ²½μ„ μ„¤μ •ν•©λ‹λ‹¤.
GPU μ„¤μ •, CUDA ν™•μΈ, Windows νΉν™” ν™κ²½ λ³€μ μ„¤μ •μ„ μν–‰ν•©λ‹λ‹¤.

μ‚¬μ©λ²•:
    python setup.py

μ£Όμμ‚¬ν•­:
    - μ΄ μ¤ν¬λ¦½νΈλ” train.py, inference.py, quantization.py μ‹¤ν–‰ μ „μ— λ¨Όμ € μ‹¤ν–‰ν•΄μ•Ό ν•©λ‹λ‹¤.
    - Windows 11 ν™κ²½μ GPU λ‚΄μ¥ λ…ΈνΈλ¶μ— μµμ ν™”λμ–΄ μμµλ‹λ‹¤.
    - λ…ΈνΈλ¶ GPUμ λ©”λ¨λ¦¬ μ μ•½μ„ κ³ λ ¤ν• μ„¤μ •μ΄ μ μ©λ©λ‹λ‹¤.
"""

import os
import sys
import torch
import platform
import subprocess
from pathlib import Path


def check_windows_environment():
    """Windows ν™κ²½ ν™•μΈ"""
    print("=== Windows ν™κ²½ ν™•μΈ ===")
    
    if platform.system() != "Windows":
        print("β οΈ  μ΄ μ¤ν¬λ¦½νΈλ” Windows ν™κ²½μ— μµμ ν™”λμ–΄ μμµλ‹λ‹¤.")
        print("   λ‹¤λ¥Έ μ΄μμ²΄μ μ—μ„λ” μΌλ¶€ κΈ°λ¥μ΄ μ ν•λ  μ μμµλ‹λ‹¤.")
    else:
        print(f"β… Windows ν™κ²½: {platform.system()} {platform.release()}")
        
        # Windows λ²„μ „ ν™•μΈ
        try:
            result = subprocess.run(['ver'], capture_output=True, text=True, shell=True)
            if result.returncode == 0:
                print(f"   Windows λ²„μ „: {result.stdout.strip()}")
        except:
            pass
    
    print()


def check_python_version():
    """Python λ²„μ „ ν™•μΈ"""
    print("=== Python λ²„μ „ ν™•μΈ ===")
    print(f"Python λ²„μ „: {sys.version}")
    
    # Python 3.8 μ΄μƒ κ¶μ¥
    if sys.version_info < (3, 8):
        print("β οΈ  κ²½κ³ : Python 3.8 μ΄μƒμ„ κ¶μ¥ν•©λ‹λ‹¤.")
    else:
        print("β… Python λ²„μ „μ΄ μ μ ν•©λ‹λ‹¤.")
    print()


def check_system_info():
    """μ‹μ¤ν… μ •λ³΄ ν™•μΈ"""
    print("=== μ‹μ¤ν… μ •λ³΄ ===")
    print(f"μ΄μμ²΄μ : {platform.system()} {platform.release()}")
    print(f"μ•„ν‚¤ν…μ²: {platform.machine()}")
    print(f"ν”„λ΅μ„Έμ„: {platform.processor()}")
    
    # λ©”λ¨λ¦¬ μ •λ³΄ ν™•μΈ (Windows)
    try:
        import psutil
        memory = psutil.virtual_memory()
        print(f"μ‹μ¤ν… λ©”λ¨λ¦¬: {memory.total / (1024**3):.1f}GB (μ‚¬μ© κ°€λ¥: {memory.available / (1024**3):.1f}GB)")
    except ImportError:
        print("λ©”λ¨λ¦¬ μ •λ³΄: psutil ν¨ν‚¤μ§€κ°€ μ„¤μΉλμ§€ μ•μ•„ ν™•μΈν•  μ μ—†μµλ‹λ‹¤.")
    
    print()


def setup_gpu_environment():
    """GPU ν™κ²½ μ„¤μ • (λ…ΈνΈλ¶ μµμ ν™”)"""
    print("=== GPU ν™κ²½ μ„¤μ • (λ…ΈνΈλ¶ μµμ ν™”) ===")
    
    # CUDA μ‚¬μ© κ°€λ¥ μ—¬λ¶€ ν™•μΈ
    if torch.cuda.is_available():
        # GPU μ •λ³΄ μ¶λ ¥
        gpu_count = torch.cuda.device_count()
        print(f"β… GPU μ‚¬μ© κ°€λ¥: {gpu_count}κ° GPU λ°κ²¬")
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
            
            # λ…ΈνΈλ¶ GPU λ©”λ¨λ¦¬ κ²½κ³ 
            if gpu_memory < 4.0:
                print(f"   β οΈ  GPU λ©”λ¨λ¦¬κ°€ μ μµλ‹λ‹¤. λ°°μΉ ν¬κΈ°λ¥Ό μ¤„μ—¬μ•Ό ν•  μ μμµλ‹λ‹¤.")
            elif gpu_memory < 8.0:
                print(f"   β οΈ  GPU λ©”λ¨λ¦¬κ°€ μ ν•μ μ…λ‹λ‹¤. λ¨λΈ ν¬κΈ°μ— μ£Όμν•μ„Έμ”.")
        
        # λ…ΈνΈλ¶ GPU μµμ ν™” μ„¤μ •
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'
        print(f"   κΈ°λ³Έ GPUλ΅ μ„¤μ •: GPU 0")
        
        # CUDA λ²„μ „ ν™•μΈ
        cuda_version = torch.version.cuda
        if cuda_version:
            print(f"   CUDA λ²„μ „: {cuda_version}")
        
        # cuDNN λ²„μ „ ν™•μΈ
        if torch.backends.cudnn.is_available():
            print(f"   cuDNN μ‚¬μ© κ°€λ¥: {torch.backends.cudnn.version()}")
        
        # λ…ΈνΈλ¶ GPU μµμ ν™” μ„¤μ •
        os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # λ””λ²„κΉ…μ„ μ„ν• λ™κΈ° μ‹¤ν–‰
        print("   CUDA_LAUNCH_BLOCKING=1 (λ””λ²„κΉ… λ¨λ“)")
        
        print("β… GPU ν™κ²½ μ„¤μ • μ™„λ£ (λ…ΈνΈλ¶ μµμ ν™”)")
        
    else:
        print("β οΈ  GPUλ¥Ό μ°Ύμ„ μ μ—†μµλ‹λ‹¤. CPUλ¥Ό μ‚¬μ©ν•©λ‹λ‹¤.")
        print("   - ν•™μµ μ†λ„κ°€ λ§¤μ° λλ¦΄ μ μμµλ‹λ‹¤.")
        print("   - λ€μ©λ‰ λ¨λΈμ κ²½μ° λ©”λ¨λ¦¬ λ¶€μ΅±μ΄ λ°μƒν•  μ μμµλ‹λ‹¤.")
        print("   - GPU λ“λΌμ΄λ²„λ¥Ό μ—…λ°μ΄νΈν•κ±°λ‚ CUDAλ¥Ό μ„¤μΉν•΄λ³΄μ„Έμ”.")
        
        # CPU μ‚¬μ© μ„¤μ •
        os.environ['CUDA_VISIBLE_DEVICES'] = ''
        print("β… CPU ν™κ²½ μ„¤μ • μ™„λ£")
    
    print()


def setup_windows_environment_variables():
    """Windows νΉν™” ν™κ²½ λ³€μ μ„¤μ •"""
    print("=== Windows νΉν™” ν™κ²½ λ³€μ μ„¤μ • ===")
    
    # PyTorch κ΄€λ ¨ ν™κ²½ λ³€μ (Windows μµμ ν™”)
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:64'  # λ…ΈνΈλ¶ GPU λ©”λ¨λ¦¬ μ μ•½ κ³ λ ¤
    print("   PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:64 (λ…ΈνΈλ¶ μµμ ν™”)")
    
    # Windows νΉν™” μ„¤μ •
    os.environ['OMP_NUM_THREADS'] = '1'  # OpenMP μ¤λ λ“ μ μ ν•
    print("   OMP_NUM_THREADS=1 (Windows μµμ ν™”)")
    
    # Transformers κ΄€λ ¨ ν™κ²½ λ³€μ
    os.environ['TRANSFORMERS_CACHE'] = './cache'
    print("   TRANSFORMERS_CACHE=./cache")
    
    # HuggingFace Hub μΊμ‹ μ„¤μ •
    os.environ['HF_HOME'] = './cache'
    print("   HF_HOME=./cache")
    
    # λ΅κ·Έ λ λ²¨ μ„¤μ • (κ²½κ³  λ©”μ‹μ§€ μ¤„μ΄κΈ°)
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'
    print("   TOKENIZERS_PARALLELISM=false")
    
    # Windows κ²½λ΅ μ„¤μ •
    os.environ['PATH'] = os.environ.get('PATH', '') + ';.\\cache'
    print("   PATHμ— cache λ””λ ‰ν† λ¦¬ μ¶”κ°€")
    
    print("β… Windows νΉν™” ν™κ²½ λ³€μ μ„¤μ • μ™„λ£")
    print()


def create_directories():
    """ν•„μ”ν• λ””λ ‰ν† λ¦¬ μƒμ„± (Windows κ²½λ΅)"""
    print("=== λ””λ ‰ν† λ¦¬ μƒμ„± ===")
    
    directories = [
        "cache",
        "model-checkpoints",
        "model-checkpoints\\kobert",
        "final-model",
        "bnb-4bit",
        "logs"  # λ΅κ·Έ νμΌ μ €μ¥μ©
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        print(f"   β… {directory}\\")
    
    print("β… λ””λ ‰ν† λ¦¬ μƒμ„± μ™„λ£")
    print()


def check_dependencies():
    """ν•„μ”ν• λΌμ΄λΈλ¬λ¦¬ ν™•μΈ (Windows μµμ ν™”)"""
    print("=== μμ΅΄μ„± λΌμ΄λΈλ¬λ¦¬ ν™•μΈ ===")
    
    required_packages = [
        'torch',
        'transformers',
        'datasets',
        'pandas',
        'numpy',
        'evaluate',
        'peft',
        'accelerate',
        'bitsandbytes'
    ]
    
    # Windowsμ—μ„ μ¶”κ°€λ΅ κ¶μ¥ν•λ” ν¨ν‚¤μ§€
    recommended_packages = [
        'psutil',  # μ‹μ¤ν… μ •λ³΄ ν™•μΈ
        'tqdm'     # μ§„ν–‰λ¥  ν‘μ‹
    ]
    
    missing_packages = []
    missing_recommended = []
    
    for package in required_packages:
        try:
            __import__(package)
            print(f"   β… {package}")
        except ImportError:
            print(f"   β {package} (μ„¤μΉ ν•„μ”)")
            missing_packages.append(package)
    
    for package in recommended_packages:
        try:
            __import__(package)
            print(f"   β… {package} (κ¶μ¥)")
        except ImportError:
            print(f"   β οΈ  {package} (κ¶μ¥, μ„¤μΉ μ•λ¨)")
            missing_recommended.append(package)
    
    if missing_packages:
        print(f"\nβ οΈ  λ‹¤μ ν•„μ ν¨ν‚¤μ§€λ“¤μ΄ μ„¤μΉλμ§€ μ•μ•μµλ‹λ‹¤:")
        for package in missing_packages:
            print(f"   - {package}")
        print("\nλ‹¤μ λ…λ Ήμ–΄λ΅ μ„¤μΉν•μ„Έμ”:")
        print("   pip install -r requirements.txt")
        print()
    else:
        print("\nβ… λ¨λ“  ν•„μ ν¨ν‚¤μ§€κ°€ μ„¤μΉλμ–΄ μμµλ‹λ‹¤.")
        
        if missing_recommended:
            print(f"\nπ’΅ λ‹¤μ κ¶μ¥ ν¨ν‚¤μ§€λ“¤μ„ μ„¤μΉν•λ©΄ λ” λ‚μ€ κ²½ν—μ„ ν•  μ μμµλ‹λ‹¤:")
            for package in missing_recommended:
                print(f"   - {package}")
            print("   pip install psutil tqdm")
            print()
    
    return len(missing_packages) == 0


def test_basic_functionality():
    """κΈ°λ³Έ κΈ°λ¥ ν…μ¤νΈ (Windows μµμ ν™”)"""
    print("=== κΈ°λ³Έ κΈ°λ¥ ν…μ¤νΈ ===")
    
    try:
        # PyTorch ν…μ¤νΈ
        x = torch.randn(2, 3)
        y = torch.randn(2, 3)
        z = x + y
        print("   β… PyTorch κΈ°λ³Έ μ—°μ‚°")
        
        # GPU ν…μ¤νΈ (GPUκ°€ μλ” κ²½μ°)
        if torch.cuda.is_available():
            x_gpu = x.cuda()
            y_gpu = y.cuda()
            z_gpu = x_gpu + y_gpu
            print("   β… GPU μ—°μ‚°")
            
            # GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰ ν™•μΈ
            gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**2
            print(f"   GPU λ©”λ¨λ¦¬ μ‚¬μ©λ‰: {gpu_memory_allocated:.1f}MB")
        
        # Transformers ν…μ¤νΈ
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained("skt/kobert-base-v1", use_fast=False)
        tokens = tokenizer("ν…μ¤νΈ λ¬Έμ¥", return_tensors="pt")
        print("   β… Transformers ν† ν¬λ‚μ΄μ €")
        
        # Windows κ²½λ΅ ν…μ¤νΈ
        test_file = Path("cache") / "test.txt"
        test_file.write_text("test")
        test_file.unlink()  # ν…μ¤νΈ νμΌ μ‚­μ 
        print("   β… Windows νμΌ μ‹μ¤ν…")
        
        print("β… κΈ°λ³Έ κΈ°λ¥ ν…μ¤νΈ μ™„λ£")
        
    except Exception as e:
        print(f"   β ν…μ¤νΈ μ‹¤ν¨: {e}")
        return False
    
    print()
    return True


def print_windows_usage_instructions():
    """Windows ν™κ²½ μ‚¬μ©λ²• μ•λ‚΄"""
    print("=== Windows ν™κ²½ μ‚¬μ©λ²• μ•λ‚΄ ===")
    print("ν™κ²½ μ„¤μ •μ΄ μ™„λ£λμ—μµλ‹λ‹¤! μ΄μ  λ‹¤μ μμ„λ΅ ν”„λ΅μ νΈλ¥Ό μ‹¤ν–‰ν•  μ μμµλ‹λ‹¤:")
    print()
    print("1. λ¨λΈ ν•™μµ:")
    print("   python train.py")
    print("   (λ…ΈνΈλ¶ GPU λ©”λ¨λ¦¬ μ μ•½μΌλ΅ μΈν•΄ λ°°μΉ ν¬κΈ°λ¥Ό μ¤„μ—¬μ•Ό ν•  μ μμµλ‹λ‹¤)")
    print()
    print("2. λ¨λΈ μ–‘μν™” (μ„ νƒμ‚¬ν•­, λ©”λ¨λ¦¬ μ μ•½):")
    print("   python quantization.py")
    print("   (λ…ΈνΈλ¶μ—μ„λ” μ–‘μν™”λ¥Ό κ¶μ¥ν•©λ‹λ‹¤)")
    print()
    print("3. λ¨λΈ μ¶”λ΅ :")
    print("   python inference.py")
    print()
    print("π’΅ Windows λ…ΈνΈλ¶ μµμ ν™” ν:")
    print("   - GPU λ©”λ¨λ¦¬κ°€ λ¶€μ΅±ν•λ©΄ train.pyμ BATCH_SIZEλ¥Ό μ¤„μ΄μ„Έμ”")
    print("   - ν•™μµ μ¤‘μ—λ” λ‹¤λ¥Έ ν”„λ΅κ·Έλ¨μ„ μΆ…λ£ν•μ—¬ GPU λ©”λ¨λ¦¬λ¥Ό ν™•λ³΄ν•μ„Έμ”")
    print("   - μ–‘μν™”λ λ¨λΈμ„ μ‚¬μ©ν•λ©΄ λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ„ ν¬κ² μ¤„μΌ μ μμµλ‹λ‹¤")
    print("   - κ° μ¤ν¬λ¦½νΈμ μ„¤μ •κ°’μ€ νμΌ ν•λ‹¨μ—μ„ μμ •ν•  μ μμµλ‹λ‹¤")
    print()


def main():
    """λ©”μΈ ν•¨μ"""
    print("π€ ν•κµ­μ–΄ μ•…μ„± λ“κΈ€ λ¶„λ¥ ν”„λ΅μ νΈ ν™κ²½ μ„¤μ • (Windows 11 + GPU λ…ΈνΈλ¶)")
    print("=" * 70)
    print()
    
    # Windows ν™κ²½ ν™•μΈ
    check_windows_environment()
    
    # Python λ²„μ „ ν™•μΈ
    check_python_version()
    
    # μ‹μ¤ν… μ •λ³΄ ν™•μΈ
    check_system_info()
    
    # GPU ν™κ²½ μ„¤μ •
    setup_gpu_environment()
    
    # Windows νΉν™” ν™κ²½ λ³€μ μ„¤μ •
    setup_windows_environment_variables()
    
    # λ””λ ‰ν† λ¦¬ μƒμ„±
    create_directories()
    
    # μμ΅΄μ„± ν™•μΈ
    dependencies_ok = check_dependencies()
    
    # κΈ°λ³Έ κΈ°λ¥ ν…μ¤νΈ
    if dependencies_ok:
        test_ok = test_basic_functionality()
    else:
        test_ok = False
    
    print("=" * 70)
    
    if dependencies_ok and test_ok:
        print("π‰ Windows ν™κ²½ μ„¤μ •μ΄ μ„±κ³µμ μΌλ΅ μ™„λ£λμ—μµλ‹λ‹¤!")
        print_windows_usage_instructions()
    else:
        print("β οΈ  ν™κ²½ μ„¤μ •μ— λ¬Έμ κ°€ μμµλ‹λ‹¤.")
        print("μμ΅΄μ„± ν¨ν‚¤μ§€λ¥Ό μ„¤μΉν•κ³  λ‹¤μ‹ μ‹¤ν–‰ν•΄μ£Όμ„Έμ”.")
        print()
        print("μ„¤μΉ λ…λ Ήμ–΄:")
        print("   pip install -r requirements.txt")
        print("   pip install psutil tqdm  # κ¶μ¥ ν¨ν‚¤μ§€")
    
    return 0 if dependencies_ok and test_ok else 1


if __name__ == "__main__":
    exit(main()) 