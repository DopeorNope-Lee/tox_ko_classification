#!/usr/bin/env python3
"""
í•œêµ­ì–´ ì•…ì„± ëŒ“ê¸€ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í•œêµ­ì–´ ëŒ“ê¸€ì„ ë¶„ì„í•˜ì—¬ ì•…ì„± ëŒ“ê¸€ê³¼ ì¼ë°˜ ëŒ“ê¸€ì„ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
KoBERT ê¸°ë°˜ ëª¨ë¸ì— LoRAë¥¼ ì ìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ fine-tuningì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
Windows 11 í™˜ê²½ì˜ GPU ë‚´ì¥ ë…¸íŠ¸ë¶ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

í•™ìŠµì„ ì‹œì‘í•˜ë ¤ë©´ ì´ íŒŒì¼ í•˜ë‹¨ì˜ ì„¤ì •ê°’ë“¤ì„ ìˆ˜ì •í•œ í›„ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.

ì‚¬ìš© ì „ì— setup.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ í™˜ê²½ì„ ì„¤ì •í•˜ì„¸ìš”.

ì£¼ì˜ì‚¬í•­:
    - ë…¸íŠ¸ë¶ GPU ë©”ëª¨ë¦¬ ì œì•½ì„ ê³ ë ¤í•˜ì—¬ ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°ê°€ ì‘ê²Œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    - GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ë©´ BATCH_SIZEë¥¼ ë” ì¤„ì´ì„¸ìš”.
    - í•™ìŠµ ì¤‘ì—ëŠ” ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•˜ì—¬ GPU ë©”ëª¨ë¦¬ë¥¼ í™•ë³´í•˜ì„¸ìš”.
"""

import numpy as np
import pandas as pd
import torch
import datasets
from pathlib import Path
from typing import Dict, Tuple

# Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from transformers import (
    Trainer, 
    TrainingArguments, 
    AutoTokenizer, 
    DataCollatorWithPadding,
    AutoConfig, 
    AutoModelForSequenceClassification
)
from peft import LoraConfig, get_peft_model
import evaluate


def load_and_preprocess_data(data_path: str = "./korean-malicious-comments-dataset/Dataset.csv"):
    """
    ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
    
    Args:
        data_path (str): ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        datasets.DatasetDict: ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹
    """
    print("=== ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ===")
    
    # CSV íŒŒì¼ ë¡œë“œ (Windows ê²½ë¡œ ì§€ì›)
    try:
        df = pd.read_csv(data_path, sep='\t')
    except:
        df = pd.read_csv(data_path)
    
    print(f"ì›ë³¸ ë°ì´í„°ì…‹ í¬ê¸°: {df.shape}")
    
    # ì»¬ëŸ¼ëª… ì„¤ì •
    df.columns = ['text', 'label']
    
    # ê²°ì¸¡ê°’ ì œê±°
    df = df.dropna().reset_index(drop=True)
    print(f"ì „ì²˜ë¦¬ í›„ ë°ì´í„°ì…‹ í¬ê¸°: {df.shape}")
    
    # ë¼ë²¨ ë¶„í¬ í™•ì¸
    print("ë¼ë²¨ ë¶„í¬:")
    print(df['label'].value_counts())
    
    # ë°ì´í„° ë¶„í•  (train: 9500, validation: 500) - í•„ìš”ì‹œ ìˆ˜ì •
    valid_df = df.sample(n=500, random_state=42)
    train_df = df.drop(valid_df.index)
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_df)}ê°œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(valid_df)}ê°œ")
    
    # HuggingFace Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    data = datasets.DatasetDict({
        'train': datasets.Dataset.from_pandas(train_df),
        'valid': datasets.Dataset.from_pandas(valid_df)
    })
    
    print("ë°ì´í„° ë¡œë”© ì™„ë£Œ\n")
    return data


def build_prompt(sentence: str) -> str:
    """
    ì…ë ¥ ë¬¸ì¥ì„ í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    
    Args:
        sentence (str): ì›ë³¸ ë¬¸ì¥
        
    Returns:
        str: í”„ë¡¬í”„íŠ¸ í˜•ì‹ì˜ ë¬¸ì¥
    """
    return (
        "ë‹¤ìŒ ë¬¸ì¥ì´ ê¸ì •ì¸ì§€ ë¶€ì •ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.\n\n"
        "### ë¬¸ì¥:\n"
        f"{sentence}"
    )


def get_dataset(data, tokenizer, max_len: int = 512):
    """
    ë°ì´í„°ì…‹ì„ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    
    Args:
        data: HuggingFace DatasetDict
        tokenizer: í† í¬ë‚˜ì´ì €
        max_len (int): ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        
    Returns:
        í† í¬ë‚˜ì´ì§•ëœ ë°ì´í„°ì…‹
    """
    def _encode(batch):
        # í…ìŠ¤íŠ¸ë¥¼ í† í°í™”
        enc = tokenizer(batch["text"],
                        truncation=True,
                        padding="max_length",
                        max_length=max_len)
        # ë¼ë²¨ì„ int64 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        enc["labels"] = np.array(batch["label"], dtype=np.int64)
        return enc

    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ í† í¬ë‚˜ì´ì§• ìˆ˜í–‰
    tokenised = data.map(_encode, batched=True, remove_columns=["text", "label"])
    # PyTorch í˜•ì‹ìœ¼ë¡œ ì„¤ì •
    tokenised.set_format("torch",
                         columns=["input_ids", "attention_mask", "labels"])
    return tokenised


def setup_model_and_tokenizer(model_path: str = "skt/kobert-base-v1"):
    """
    ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •
    
    Args:
        model_path (str): ëª¨ë¸ ê²½ë¡œ
        
    Returns:
        tuple: (model, tokenizer)
    """
    print("=== ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„¤ì • ===")
    
    # ëª¨ë¸ ì„¤ì • (2ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜)
    config = AutoConfig.from_pretrained(
        model_path, 
        num_labels=2, 
        problem_type="single_label_classification"
    )
    
    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForSequenceClassification.from_pretrained(model_path, config=config)
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ (KoBERTëŠ” fast tokenizerë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ)
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
    
    # ë¼ë²¨ ë§¤í•‘ ì„¤ì •
    model.config.id2label = {0: "toxic", 1: "none"}
    model.config.label2id = {v: k for k, v in model.config.id2label.items()}
    
    print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
    print(f"ë¼ë²¨ ë§¤í•‘: 0 -> {model.config.id2label[0]}, 1 -> {model.config.id2label[1]}")
    
    return model, tokenizer


def setup_lora(model, r: int = 16, lora_alpha: int = 16, lora_dropout: float = 0.1):
    """
    LoRA ì„¤ì • ë° ì ìš©
    
    Args:
        model: ê¸°ë³¸ ëª¨ë¸
        r (int): LoRAì˜ rank
        lora_alpha (int): LoRAì˜ alpha ê°’
        lora_dropout (float): Dropout ë¹„ìœ¨
        
    Returns:
        LoRAê°€ ì ìš©ëœ ëª¨ë¸
    """
    print("=== LoRA ì„¤ì • ===")
    
    # KoBERT ëª¨ë¸ì˜ LoRA íƒ€ê²Ÿ ëª¨ë“ˆ ì„¤ì •
    targets = ["query", "key", "value"]
    
    # LoRA ì„¤ì •
    lora_cfg = LoraConfig(
        r=r,
        lora_alpha=lora_alpha,
        target_modules=targets,
        lora_dropout=lora_dropout,
        bias="none",
        task_type="SEQUENCE_CLASSIFICATION"
    )
    
    # LoRA ì ìš©
    model = get_peft_model(model, lora_cfg)
    
    # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ ì¶œë ¥
    model.print_trainable_parameters()
    
    print("LoRA ì„¤ì • ì™„ë£Œ\n")
    return model


class SmartCollator(DataCollatorWithPadding):
    """ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ì½œë ˆì´í„°"""
    
    def __call__(self, features):
        batch = super().__call__(features)
        return batch


def compute_metrics(eval_pred):
    """
    í‰ê°€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë©”íŠ¸ë¦­ì„ ê³„ì‚°
    
    Args:
        eval_pred: (logits, labels) íŠœí”Œ
        
    Returns:
        Dict: ì •í™•ë„ì™€ F1 ì ìˆ˜
    """
    logits, labels = eval_pred
    
    preds = logits.argmax(axis=-1)
    labels = labels.astype(np.int64)
    
    # ì •í™•ë„ ê³„ì‚°
    accuracy_metric = evaluate.load("accuracy")
    acc = accuracy_metric.compute(predictions=preds, references=labels)["accuracy"]
    
    # F1 ì ìˆ˜ ê³„ì‚° (weighted average)
    f1_metric = evaluate.load("f1")
    f1 = f1_metric.compute(predictions=preds, references=labels, average="weighted")["f1"]
    
    return {"accuracy": acc, "f1": f1}


def train_model(model, tokenizer, data, max_len: int = 512, batch_size: int = 32, 
                eval_batch_size: int = 8, epochs: int = 20, learning_rate: float = 5e-5,
                output_dir: str = "model-checkpoints/kobert"):
    """
    ëª¨ë¸ í•™ìŠµ ìˆ˜í–‰ (ë…¸íŠ¸ë¶ GPU ìµœì í™”)
    
    Args:
        model: í•™ìŠµí•  ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        data: ë°ì´í„°ì…‹
        max_len (int): ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        batch_size (int): í›ˆë ¨ ë°°ì¹˜ í¬ê¸° (ë…¸íŠ¸ë¶ GPU ë©”ëª¨ë¦¬ ì œì•½ ê³ ë ¤)
        eval_batch_size (int): í‰ê°€ ë°°ì¹˜ í¬ê¸°
        epochs (int): í›ˆë ¨ ì—í¬í¬ ìˆ˜
        learning_rate (float): í•™ìŠµë¥ 
        output_dir (str): ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        
    Returns:
        Trainer: í•™ìŠµëœ íŠ¸ë ˆì´ë„ˆ ê°ì²´
    """
    print("=== ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ë…¸íŠ¸ë¶ GPU ìµœì í™”) ===")
    
    # ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§•
    ds = get_dataset(data, tokenizer, max_len)
    print("ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• ì™„ë£Œ")
    
    # í† í¬ë‚˜ì´ì € ì„ë² ë”© í¬ê¸° ì¡°ì •
    model.resize_token_embeddings(len(tokenizer))
    model.config.pad_token_id = tokenizer.pad_token_id
    
    # GPU ë©”ëª¨ë¦¬ í™•ì¸
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB")
        
        # GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì • ì œì•ˆ
        if gpu_memory < 4.0 and batch_size > 16:
            print(f"âš ï¸  GPU ë©”ëª¨ë¦¬ê°€ ì ìŠµë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ 16 ì´í•˜ë¡œ ì¤„ì´ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        elif gpu_memory < 8.0 and batch_size > 32:
            print(f"âš ï¸  GPU ë©”ëª¨ë¦¬ê°€ ì œí•œì ì…ë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ 32 ì´í•˜ë¡œ ì¤„ì´ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
    
    # í•™ìŠµ ì¸ì ì„¤ì • (ë…¸íŠ¸ë¶ ìµœì í™”)
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=eval_batch_size,
        num_train_epochs=epochs,
        learning_rate=learning_rate,
        fp16=True,  # 16ë¹„íŠ¸ ì •ë°€ë„ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        eval_strategy="steps",
        save_strategy="steps",
        logging_steps=1,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        save_steps=50,
        eval_steps=50,
        warmup_steps=100,
        weight_decay=0.01,
        # ë…¸íŠ¸ë¶ ìµœì í™” ì„¤ì •
        dataloader_pin_memory=False,  # Windowsì—ì„œ ë©”ëª¨ë¦¬ ì ˆì•½
        remove_unused_columns=False,  # Windows í˜¸í™˜ì„±
        report_to=None,  # ë¡œê¹… ë¹„í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
    )
    
    # ë°ì´í„° ì½œë ˆì´í„° ì„¤ì •
    data_collator = SmartCollator(tokenizer, return_tensors="pt")
    
    # Trainer ê°ì²´ ìƒì„±
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=ds["train"],
        eval_dataset=ds["valid"],
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )
    
    # ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
    trainer.train()
    
    print("ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    
    return trainer


def evaluate_model(trainer):
    """ëª¨ë¸ í‰ê°€ ë° ê²°ê³¼ ì¶œë ¥"""
    print("=== ëª¨ë¸ í‰ê°€ ===")
    
    # ìµœì¢… ëª¨ë¸ í‰ê°€
    final_results = trainer.evaluate()
    
    print("\n=== ìµœì¢… í‰ê°€ ê²°ê³¼ ===")
    print(f"í‰ê°€ ì†ì‹¤: {final_results['eval_loss']:.4f}")
    print(f"ì •í™•ë„: {final_results['eval_accuracy']:.4f}")
    print(f"F1 ì ìˆ˜: {final_results['eval_f1']:.4f}")
    
    return final_results


def save_model(trainer, tokenizer, save_path: str = "final-model"):
    """ëª¨ë¸ ì €ì¥"""
    print(f"=== ëª¨ë¸ ì €ì¥: {save_path} ===")
    
    trainer.save_model(save_path)
    tokenizer.save_pretrained(save_path)
    
    print(f"ëª¨ë¸ì´ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def test_predictions(model, tokenizer, test_texts: list):
    """ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸"""
    print("=== ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ===")
    
    def predict_text(text):
        # í”„ë¡¬í”„íŠ¸ ì ìš©
        prompt_text = build_prompt(text)
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(prompt_text, return_tensors="pt", truncation=True, max_length=512)
        
        # ì˜ˆì¸¡
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.softmax(outputs.logits, dim=-1)
            predicted_class = torch.argmax(predictions, dim=-1).item()
        
        return model.config.id2label[predicted_class]
    
    for text in test_texts:
        result = predict_text(text)
        print(f"í…ìŠ¤íŠ¸: {text}")
        print(f"ì˜ˆì¸¡: {result}\n")


# ============================================================================
# í•™ìŠµ ì„¤ì •ê°’ - ì—¬ê¸°ì„œ ì§ì ‘ ìˆ˜ì •í•˜ì„¸ìš” (Windows ë…¸íŠ¸ë¶ ìµœì í™”)
# ============================================================================

# ë°ì´í„° ì„¤ì •
DATA_PATH = "./korean-malicious-comments-dataset/Dataset.csv"

# ëª¨ë¸ ì„¤ì •
MODEL_PATH = "skt/kobert-base-v1"
MAX_LEN = 512

# í•™ìŠµ ì„¤ì • (ë…¸íŠ¸ë¶ GPU ë©”ëª¨ë¦¬ ì œì•½ ê³ ë ¤)
BATCH_SIZE = 32      # ë…¸íŠ¸ë¶ GPU ë©”ëª¨ë¦¬ ì œì•½ìœ¼ë¡œ ì‘ê²Œ ì„¤ì • (ê¸°ì¡´: 128)
EVAL_BATCH_SIZE = 8  # í‰ê°€ ë°°ì¹˜ í¬ê¸°ë„ ì‘ê²Œ ì„¤ì • (ê¸°ì¡´: 16)
EPOCHS = 20
LEARNING_RATE = 5e-5

# LoRA ì„¤ì •
LORA_R = 16
LORA_ALPHA = 16
LORA_DROPOUT = 0.1

# ì €ì¥ ê²½ë¡œ ì„¤ì • (Windows ê²½ë¡œ)
OUTPUT_DIR = "model-checkpoints\\kobert"
SAVE_PATH = "final-model"

# í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
TEST_TEXTS = [
    "ì•ˆë…•í•˜ì„¸ìš”! ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”.",
    "ì´ëŸ° ë§ì€ í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.",
    "ì •ë§ ë©‹ì§„ í”„ë¡œì íŠ¸ë„¤ìš”!"
]

# ============================================================================
# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
# ============================================================================

if __name__ == "__main__":
    print("ğŸš€ í•œêµ­ì–´ ì•…ì„± ëŒ“ê¸€ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ (Windows 11 + GPU ë…¸íŠ¸ë¶)")
    print("=" * 60)
    print()
    
    # ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
    data = load_and_preprocess_data(DATA_PATH)
    
    # í…ìŠ¤íŠ¸ì— í”„ë¡¬í”„íŠ¸ ì ìš©
    data = data.map(lambda x: {"text": build_prompt(x["text"])})
    
    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„¤ì •
    model, tokenizer = setup_model_and_tokenizer(MODEL_PATH)
    
    # LoRA ì„¤ì •
    model = setup_lora(model, LORA_R, LORA_ALPHA, LORA_DROPOUT)
    
    # ëª¨ë¸ í•™ìŠµ
    trainer = train_model(
        model, tokenizer, data, 
        max_len=MAX_LEN,
        batch_size=BATCH_SIZE,
        eval_batch_size=EVAL_BATCH_SIZE,
        epochs=EPOCHS,
        learning_rate=LEARNING_RATE,
        output_dir=OUTPUT_DIR
    )
    
    # ëª¨ë¸ í‰ê°€
    results = evaluate_model(trainer)
    
    # ëª¨ë¸ ì €ì¥
    save_model(trainer, tokenizer, SAVE_PATH)
    
    # ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    test_predictions(model, tokenizer, TEST_TEXTS)
    
    print("=" * 60)
    print("=== í•™ìŠµ ì™„ë£Œ ===")
    print(f"ìµœì¢… ì •í™•ë„: {results['eval_accuracy']:.4f}")
    print(f"ìµœì¢… F1 ì ìˆ˜: {results['eval_f1']:.4f}")
    print()
    print("ğŸ’¡ ë…¸íŠ¸ë¶ ìµœì í™” íŒ:")
    print("   - GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ë©´ BATCH_SIZEë¥¼ ë” ì¤„ì´ì„¸ìš” (16 ë˜ëŠ” 8)")
    print("   - í•™ìŠµ ì¤‘ì—ëŠ” ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•˜ì—¬ GPU ë©”ëª¨ë¦¬ë¥¼ í™•ë³´í•˜ì„¸ìš”")
    print("   - ì–‘ìí™”ë¥¼ ìœ„í•´ quantization.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”") 